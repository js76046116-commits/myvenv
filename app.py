import streamlit as st
import os
import json
import itertools
from sentence_transformers import CrossEncoder 

from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.retrievers import BM25Retriever
from langchain_core.prompts import ChatPromptTemplate 
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document

# ==========================================================
# [0] í˜ì´ì§€ ë° ê²½ë¡œ ì„¤ì •
# ==========================================================
st.set_page_config(page_title="ê±´ì„¤ CM AI ê²€ìƒ‰ ì—”ì§„", page_icon="ğŸ—ï¸", layout="wide")

if "GOOGLE_API_KEY" not in os.environ:
    pass # Streamlit Cloud Secrets ì‚¬ìš©

# âš ï¸ [í•µì‹¬] ë¶„í• ëœ DB ê²½ë¡œ ë° ì›ë³¸ ë°ì´í„° ê²½ë¡œ
DB_PATH_1 = "./chroma_db_part1"
DB_PATH_2 = "./chroma_db_part2"
JSON_DATA_PATH = "./legal_data_total_vlm.json"

# ì „ì—­ ë³€ìˆ˜ (Lookupìš©)
RAW_DATA = []

# ==========================================================
# [1] ì‹œìŠ¤í…œ ë¡œë”© (Dual DB + Lookup + Hybrid)
# ==========================================================
class SimpleHybridRetriever:
    def __init__(self, bm25, chroma1, chroma2, raw_data):
        self.bm25 = bm25
        self.chroma1 = chroma1
        self.chroma2 = chroma2
        self.raw_data = raw_data
        
    def invoke(self, query):
        # 1. BM25 ê²€ìƒ‰ (í…ìŠ¤íŠ¸ ë§¤ì¹­)
        docs_bm25 = self.bm25.invoke(query)
        
        # 2. Chroma ê²€ìƒ‰ (ë‘ DB ë™ì‹œ ê²€ìƒ‰)
        docs_c1 = self.chroma1.invoke(query)
        docs_c2 = self.chroma2.invoke(query)
        
        # 3. ID -> í…ìŠ¤íŠ¸ ë³€í™˜ (Lookup)
        real_docs_chroma = []
        for doc in (docs_c1 + docs_c2):
            try:
                idx = int(doc.page_content) 
                original_item = self.raw_data[idx] 
                
                content = original_item.get('content', '').strip()
                source = original_item.get('source', '').strip()
                article = original_item.get('article', '').strip()
                full_text = f"[{source}] {content}"
                
                new_doc = Document(page_content=full_text, metadata={"source": source, "article": article})
                real_docs_chroma.append(new_doc)
            except:
                continue

        # 4. ì¤‘ë³µ ì œê±° ë° ê²°í•©
        combined = []
        seen_ids = set()
        
        for d in itertools.chain(docs_bm25, real_docs_chroma):
            key = d.page_content[:30] 
            if key not in seen_ids:
                combined.append(d)
                seen_ids.add(key)
                
        return combined[:200]

@st.cache_resource
def load_search_system():
    global RAW_DATA
    
    # 1. JSON ë¡œë“œ
    if not os.path.exists(JSON_DATA_PATH):
        st.error("âŒ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. GitHubì— ì˜¬ë ¤ì£¼ì„¸ìš”.")
        st.stop()
    with open(JSON_DATA_PATH, 'r', encoding='utf-8') as f:
        RAW_DATA = json.load(f)

    # 2. Chroma DB ë¡œë“œ
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    if not os.path.exists(DB_PATH_1) or not os.path.exists(DB_PATH_2):
        st.error("âŒ ë¶„í• ëœ DB í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    store1 = Chroma(persist_directory=DB_PATH_1, embedding_function=embeddings, collection_name="construction_laws")
    retriever1 = store1.as_retriever(search_kwargs={"k": 100})

    store2 = Chroma(persist_directory=DB_PATH_2, embedding_function=embeddings, collection_name="construction_laws")
    retriever2 = store2.as_retriever(search_kwargs={"k": 100})

    # 3. BM25 ìƒì„±
    docs = []
    for item in RAW_DATA:
        content = item.get('content', '').strip()
        source = item.get('source', '').strip()
        if not content: continue
        doc = Document(page_content=f"[{source}] {content}", metadata={"source": source, "article": item.get('article', '')})
        docs.append(doc)
    
    bm25_retriever = BM25Retriever.from_documents(docs)
    bm25_retriever.k = 150

    # 4. ê²°í•©
    hybrid_retriever = SimpleHybridRetriever(bm25_retriever, retriever1, retriever2, RAW_DATA)
    
    # [ìˆ˜ì •] ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ê°€ë²¼ìš´ ëª¨ë¸(TinyBERT)ë¡œ êµì²´
    reranker = CrossEncoder("cross-encoder/ms-marco-TinyBERT-L-2-v2", model_kwargs={"torch_dtype": "auto"})

    return hybrid_retriever, reranker

with st.spinner("ğŸš€ AI ì—”ì§„(Dual DB) ì‹œë™ ì¤‘..."):
    hybrid_retriever, reranker_model = load_search_system()

# LLM
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)

# ==========================================================
# [2] RAG ë¡œì§
# ==========================================================
spacing_chain = ChatPromptTemplate.from_template("""
    ë‹¹ì‹ ì€ **í•œêµ­ì–´ ë„ì–´ì“°ê¸° êµì • ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.
    ì…ë ¥: {question}
    ê·œì¹™: ì„¤ëª… ì—†ì´ **êµì •ëœ ë¬¸ì¥ë§Œ** ì¶œë ¥í•˜ì„¸ìš”.
    êµì •ëœ ë¬¸ì¥:""").pipe(llm).pipe(StrOutputParser())

hyde_chain = ChatPromptTemplate.from_template("""
    ë‹¹ì‹ ì€ ê±´ì„¤ ë¶„ì•¼ **ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.
    ì…ë ¥: {question}
    ê·œì¹™: ë¼ë²¨/ì„¤ëª… ê¸ˆì§€. ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ í™•ì¥í•œ **ë‹¨ì–´ë“¤ë§Œ** ë‚˜ì—´. ì½¤ë§ˆ(,) êµ¬ë¶„.
    """).pipe(llm).pipe(StrOutputParser())

answer_prompt = ChatPromptTemplate.from_messages([
    ("system", """
    ë‹¹ì‹ ì€ ê±´ì„¤ ê¸°ì¤€ì„ ì°¾ì•„ì£¼ëŠ” **ì „ë¬¸ ì—”ì§€ë‹ˆì–´**ì…ë‹ˆë‹¤.
    [Context]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    [ì‘ì„± ì›ì¹™]
    1. **ì›ë¬¸ ì¡´ì¤‘**: ëª©ë¡(â‘ , â‘¡...)ì€ ìš”ì•½í•˜ì§€ ë§ê³  **ê·¸ëŒ€ë¡œ ë°œì·Œ**í•˜ì‹­ì‹œì˜¤.
    2. **í•´ì„**: ì„œì‹(( )%)ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì˜ë¯¸ë¥¼ í’€ì–´ì„œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
    3. **ì¶œì²˜**: ëì— [ë¬¸ì„œëª…] í•„ìˆ˜ í‘œê¸°.
    [Context]
    {context}
    """),
    ("human", "ì§ˆë¬¸: {question}")
])

def retrieve_and_rerank(query):
    initial_docs = hybrid_retriever.invoke(query)
    if not initial_docs: return []
    
    pairs = [[query, doc.page_content] for doc in initial_docs]
    scores = []
    batch_size = 16 # ê°€ë²¼ìš´ ëª¨ë¸ì´ë¼ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë ¤ë„ ë¨
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i : i + batch_size]
        batch_scores = reranker_model.predict(batch)
        scores.extend(batch_scores)
        
    scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in scored_docs[:50]]

def format_docs(docs):
    return "\n\n".join([f"<ì¶œì²˜: {d.metadata.get('source')} / {d.metadata.get('article')}>\n{d.page_content}" for d in docs])

rag_chain = (
    {
        "context": RunnableLambda(retrieve_and_rerank) | format_docs,
        "question": RunnablePassthrough() 
    }
    | answer_prompt
    | llm
    | StrOutputParser()
)

# ==========================================================
# [3] ì›¹ UI
# ==========================================================
st.title("ğŸ—ï¸ ê±´ì„¤ CM ì „ë¬¸ AI")
st.caption("ğŸš€ 1ì°¨ ì§êµ¬ ê²€ìƒ‰(Direct) í›„ â†’ ì›í•˜ë©´ HyDE ì‹¬ì¸µ ê²€ìƒ‰(Expansion)ìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # [Phase 1] 1ì°¨ ê²€ìƒ‰
        with st.status("ğŸ” 1ì°¨ ì •ë°€ ê²€ìƒ‰ ì§„í–‰ ì¤‘...", expanded=True) as status:
            st.write("ğŸ”§ ë„ì–´ì“°ê¸° êµì • ì¤‘...")
            corrected_query = spacing_chain.invoke({"question": prompt})
            st.write(f"-> êµì •ë¨: `{corrected_query}`")
            
            st.write("ğŸ“š ë¬¸ì„œ í†µí•© ê²€ìƒ‰ ì¤‘...")
            response_1 = rag_chain.invoke(corrected_query)
            status.update(label="âœ… 1ì°¨ ê²€ìƒ‰ ì™„ë£Œ!", state="complete", expanded=False)
        
        st.markdown("### ğŸ¤– 1ì°¨ ë‹µë³€")
        st.markdown(response_1)
        st.session_state.messages.append({"role": "assistant", "content": f"### ğŸ¤– 1ì°¨ ë‹µë³€\n{response_1}"})

        # [Phase 2] HyDE í™•ì¥ (ë¡œë”© UI ê°œì„ )
        with st.expander("ğŸ¤” ë‹µë³€ì´ ë¶€ì¡±í•œê°€ìš”? (ì‹¬ì¸µ ê²€ìƒ‰)"):
            if st.button("ğŸš€ HyDE ì‹¬ì¸µ ê²€ìƒ‰ ì‹¤í–‰"):
                # [ìˆ˜ì •] 1ì°¨ ê²€ìƒ‰ì²˜ëŸ¼ 'status' ë°•ìŠ¤ë¥¼ ì‚¬ìš©í•´ ì§„í–‰ìƒí™© í‘œì‹œ
                with st.status("ğŸ§  ì „ë¬¸ê°€ ëª¨ë“œ(HyDE) ê°€ë™ ì¤‘...", expanded=True) as status_2:
                    st.write("ğŸ’¡ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ í‚¤ì›Œë“œë¥¼ í™•ì¥í•©ë‹ˆë‹¤...")
                    hyde_keywords = hyde_chain.invoke({"question": corrected_query})
                    final_query = f"{corrected_query} {hyde_keywords}"
                    st.write(f"-> í™•ì¥ëœ ê²€ìƒ‰ì–´: `{final_query}`")
                    
                    st.write("ğŸš€ í™•ì¥ëœ ë²”ìœ„ë¡œ ì¬ê²€ìƒ‰ ë° ì •ë°€ ì‹¬ì‚¬ ì¤‘...")
                    response_2 = rag_chain.invoke(final_query)
                    status_2.update(label="âœ… ì‹¬ì¸µ ê²€ìƒ‰ ì™„ë£Œ!", state="complete", expanded=False)

                st.success(f"í™•ì¥ëœ í‚¤ì›Œë“œ: {hyde_keywords}")
                st.markdown("---")
                st.markdown("### ğŸ¤– 2ì°¨ ìƒì„¸ ë‹µë³€ (HyDE)")
                st.markdown(response_2)
                
                st.session_state.messages.append({"role": "assistant", "content": f"### ğŸ¤– 2ì°¨ ìƒì„¸ ë‹µë³€ (HyDE)\n{response_2}"})