import streamlit as st
import os
import json
import itertools
import base64
import tempfile
import platform 
import time

# [í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬]
from pdf2image import convert_from_path
from sentence_transformers import CrossEncoder 
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory
from langchain_community.retrievers import BM25Retriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage

# ==========================================================
# [0] ê¸°ë³¸ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ==========================================================
st.set_page_config(page_title="ê±´ì„¤ CM AI íŒŒíŠ¸ë„ˆ", page_icon="ğŸ—ï¸", layout="wide")

# 1. API í‚¤ ì„¤ì •
if "GOOGLE_API_KEY" in st.secrets:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
elif "GOOGLE_API_KEY" in os.environ:
    GOOGLE_API_KEY = os.environ["GOOGLE_API_KEY"]
else:
    st.error("ğŸš¨ Google API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

# 2. Poppler ê²½ë¡œ (ì‚¬ìš©ì í™˜ê²½)
system_name = platform.system()
if system_name == "Windows":
    # ì‚¬ìš©ìì˜ ë¡œì»¬ í™˜ê²½ ê²½ë¡œ (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
    POPPLER_PATH = r"C:\Users\owner\myvenv\Release-25.12.0-0\poppler-25.12.0\Library\bin"
else:
    POPPLER_PATH = None 

# 3. ë°ì´í„° ê²½ë¡œ
DB_PATH_1 = "./chroma_db_part1"
DB_PATH_2 = "./chroma_db_part2"
JSON_DATA_PATH = "./legal_data_total_vlm.json"
RAW_DATA = []
MODEL_NAME = "models/gemini-2.5-flash" 

# ==========================================================
# [1] ê²€ìƒ‰ ì—”ì§„ ë¡œë”© (Retriever & Reranker)
# ==========================================================
class SimpleHybridRetriever:
    """BM25 + ChromaDB í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°"""
    def __init__(self, bm25, chroma1, chroma2, raw_data):
        self.bm25 = bm25
        self.chroma1 = chroma1
        self.chroma2 = chroma2
        self.raw_data = raw_data
        
    def invoke(self, query):
        docs_bm25 = self.bm25.invoke(query)
        docs_c1 = self.chroma1.invoke(query)
        docs_c2 = self.chroma2.invoke(query)
        
        real_docs_chroma = []
        for doc in (docs_c1 + docs_c2):
            try:
                idx = int(doc.page_content) 
                original_item = self.raw_data[idx] 
                full_text = f"[{original_item['source']}] {original_item['content']}"
                new_doc = Document(page_content=full_text, metadata={"source": original_item['source'], "article": original_item['article']})
                real_docs_chroma.append(new_doc)
            except:
                continue

        combined = []
        seen_ids = set()
        for d in itertools.chain(docs_bm25, real_docs_chroma):
            key = d.page_content[:30]
            if key not in seen_ids:
                combined.append(d)
                seen_ids.add(key)
        return combined[:200]

@st.cache_resource
def load_search_system():
    global RAW_DATA
    if not os.path.exists(JSON_DATA_PATH): return None, None
    with open(JSON_DATA_PATH, 'r', encoding='utf-8') as f: RAW_DATA = json.load(f)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=GOOGLE_API_KEY)
    
    if not os.path.exists(DB_PATH_1): return None, None

    store1 = Chroma(persist_directory=DB_PATH_1, embedding_function=embeddings, collection_name="construction_laws")
    store2 = Chroma(persist_directory=DB_PATH_2, embedding_function=embeddings, collection_name="construction_laws")
    
    docs = [Document(page_content=f"[{i['source']}] {i['content']}", metadata={"source": i['source']}) for i in RAW_DATA if i['content']]
    bm25 = BM25Retriever.from_documents(docs)
    bm25.k = 100
    
    hybrid = SimpleHybridRetriever(bm25, store1.as_retriever(), store2.as_retriever(), RAW_DATA)
    reranker = CrossEncoder("cross-encoder/ms-marco-TinyBERT-L-2-v2")
    return hybrid, reranker

with st.spinner("ğŸš€ AI ì‹œìŠ¤í…œ ë¡œë”© ì¤‘..."):
    hybrid_retriever, reranker_model = load_search_system()
    if not hybrid_retriever:
        st.error("ë°ì´í„° íŒŒì¼ì´ë‚˜ DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

# LLM ì„¤ì •
safety = {HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE}
llm_text = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.1, google_api_key=GOOGLE_API_KEY, safety_settings=safety)
llm_vision = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0, google_api_key=GOOGLE_API_KEY, safety_settings=safety)

# ==========================================================
# [2] RAG ë¡œì§ (í•­ìƒ Deep RAG ì‚¬ìš©)
# ==========================================================
# (1) ì¿¼ë¦¬ í™•ì¥
expansion_chain = ChatPromptTemplate.from_template(
    "ê±´ì„¤ ê²€ìƒ‰ ìµœì í™” AIì…ë‹ˆë‹¤. '{question}'ì— ëŒ€í•œ ê²€ìƒ‰ì–´ 3ê°œë¥¼ 'ì§ˆë¬¸|í‚¤ì›Œë“œ1,í‚¤ì›Œë“œ2,í‚¤ì›Œë“œ3' í˜•ì‹ìœ¼ë¡œ ë§Œë“œì„¸ìš”."
) | llm_text | StrOutputParser()

def get_expanded_queries(query):
    try:
        res = expansion_chain.invoke({"question": query})
        if "|" in res: return [res.split("|")[0]] + res.split("|")[1].split(",")
        return [query]
    except: return [query]

# (2) ë¬¸ì„œ ê²€ìƒ‰ ë° ìˆœìœ„ ì¬ì¡°ì • (Deep Only)
def retrieve_docs_deep(query):
    # í•­ìƒ 5ë‹¨ê³„ ì‹¬ì¸µ ê²€ìƒ‰ ìˆ˜í–‰ (í™•ì¥ + Top-50 + Rerank)
    queries = get_expanded_queries(query)
    top_k = 50
    
    all_docs = []
    seen = set()
    for q in queries:
        for doc in hybrid_retriever.invoke(q):
            if doc.page_content not in seen:
                all_docs.append(doc)
                seen.add(doc.page_content)
                
    if not all_docs: return []
    
    # Rerank ìˆ˜í–‰ (ë°°ì¹˜ ì²˜ë¦¬)
    pairs = [[query, d.page_content] for d in all_docs]
    scores = []
    for i in range(0, len(pairs), 32):
        batch = pairs[i : i+32]
        scores.extend(reranker_model.predict(batch))
    scored = sorted(zip(all_docs, scores), key=lambda x: x[1], reverse=True)
    return [d for d, s in scored[:top_k]]

# (3) ë‹µë³€ ìƒì„±
answer_prompt = ChatPromptTemplate.from_messages([
    ("system", """
    ë‹¹ì‹ ì€ ê±´ì„¤ CM ê¸°ìˆ ì‚¬ì…ë‹ˆë‹¤.
    [Context]ë¥¼ ê·¼ê±°ë¡œ ë‹µë³€í•˜ë˜, ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ê³µí•™ì  ì§€ì‹ê³¼ ì‹œë°©ì„œ ê¸°ì¤€ì„ í™œìš©í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ì¡°ì–¸í•˜ì„¸ìš”.
    ì ˆëŒ€ "ëª¨ë¦…ë‹ˆë‹¤"ë¼ê³  ëë‚´ì§€ ë§ˆì„¸ìš”.
    \n[Context]\n{context}
    """),
    ("human", "{question}")
])
def format_docs(docs): return "\n\n".join([d.page_content for d in docs])

# ==========================================================
# [3] Vision AI & ë³´ê³ ì„œ ë¡œì§
# ==========================================================
def analyze_page_detail(image_base64, query):
    msg = HumanMessage(content=[
        {"type": "text", "text": f"ê±´ì„¤ ì „ë¬¸ê°€ë¡œì„œ ë„ë©´ì„ ë³´ê³  '{query}'ì— ëŒ€í•´ ë¶„ì„í•˜ì„¸ìš”. ë¬¸ì œì ì„ ì§§ê³  ëª…í™•í•˜ê²Œ(1~2ë¬¸ì¥) ì§€ì í•˜ì„¸ìš”."},
        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
    ])
    try: return llm_vision.invoke([msg]).content
    except: return "ë¶„ì„ ë¶ˆê°€"

def generate_consolidated_report(filename, page_results):
    # í˜ì´ì§€ë³„ ë‚´ìš©ì„ í•©ì³ì„œ LLMì—ê²Œ ì¤Œ
    raw_text = "\n".join([f"Page {p['page']}: {p['content']}" for p in page_results])
    
    prompt = f"""
    ë‹¹ì‹ ì€ CMë‹¨ì¥ì…ë‹ˆë‹¤. '{filename}' ë„ë©´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì¢…í•© ë¦¬í¬íŠ¸**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    1. ì „ì²´ì ì¸ ì‹œê³µ ë¦¬ìŠ¤í¬ ì´í‰
    2. ê³µì¢…ë³„ ì£¼ìš” ê´€ë¦¬ í¬ì¸íŠ¸
    (â€» ê°œë³„ í˜ì´ì§€ ë‚´ìš©ì€ ì—¬ê¸°ì„œ ë‚˜ì—´í•˜ì§€ ë§ê³ , ì „ì²´ì ì¸ íë¦„ë§Œ ìš”ì•½í•˜ì„¸ìš”.)
    
    [ë¶„ì„ ë°ì´í„°]
    {raw_text}
    """
    summary = llm_text.invoke(prompt).content
    
    # [í•µì‹¬] í˜ì´ì§€ë³„ ë¦¬ìŠ¤íŠ¸ëŠ” ê°•ì œë¡œ ë¶™ì„
    appendix = "\n\n---\n### ğŸ” [ë¶€ë¡] í˜ì´ì§€ë³„ ì§„ë‹¨ ìš”ì•½\n"
    for p in page_results:
        clean_msg = p['content'].replace('\n', ' ').strip()
        appendix += f"- **Page {p['page']}**: {clean_msg}\n"
        
    return summary + appendix

# ==========================================================
# [4] UI êµ¬ì„±
# ==========================================================
# ì„¸ì…˜ ìƒíƒœ
if "messages" not in st.session_state: st.session_state.messages = []
if "processed_files" not in st.session_state: st.session_state.processed_files = set()
if "current_image_base64" not in st.session_state: st.session_state.current_image_base64 = None

# --- ì‚¬ì´ë“œë°”: ë™ì  ëª¨ë“œ ì„ íƒ ---
with st.sidebar:
    st.header("ğŸ“‚ ë„ë©´ íˆ¬ì…êµ¬")
    uploaded_files = st.file_uploader("PDF ë„ë©´ ì—…ë¡œë“œ", type=["pdf"], accept_multiple_files=True)
    st.markdown("---")
    
    # [ìˆ˜ì •ëœ ë¡œì§]
    if uploaded_files:
        st.subheader("ğŸ¤– ì§ˆë¬¸ ì„¤ì •")
        # Case A: ë„ë©´ ìˆìŒ -> 'ë„ë©´ ë³´ê¸°' vs 'ë²•ê·œ ì°¾ê¸°' ì„ íƒ
        search_mode = st.radio(
            "ëª¨ë“œ ì„ íƒ", 
            ["ğŸ“‚ ë„ë©´ ê´€ë ¨ ì§ˆë¬¸", "âš–ï¸ ê±´ì¶• ë²•ê·œ ê²€ìƒ‰"],
            help="ğŸ“‚ ë„ë©´ ê´€ë ¨: ë³´ê³  ìˆëŠ” ë„ë©´ ë‚´ìš© ë¶„ì„\nâš–ï¸ ë²•ê·œ ê²€ìƒ‰: ë„ë©´ ë¬´ì‹œí•˜ê³  ë²•ê·œ DB ì‹¬ì¸µ ê²€ìƒ‰"
        )
    else:
        # Case B: ë„ë©´ ì—†ìŒ -> ë²„íŠ¼ ìˆ¨ê¹€ + ì‹¬ì¸µ ëª¨ë“œ ê³ ì •
        search_mode = "âš–ï¸ ê±´ì¶• ë²•ê·œ ê²€ìƒ‰"
        st.info("ğŸ’¡ **ì‹¬ì¸µ ë²•ê·œ ê²€ìƒ‰ ëª¨ë“œ**ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n(Query Expansion + Rerank ìë™ ì ìš©)")

# --- ë„ë©´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ---
if uploaded_files:
    for f in uploaded_files:
        if f.name not in st.session_state.processed_files:
            with st.status(f"ğŸ“„ '{f.name}' ì •ë°€ ë¶„ì„ ì¤‘...", expanded=True) as status:
                # 1. ë³€í™˜
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                    tmp.write(f.read())
                    path = tmp.name
                
                try:
                    pages = convert_from_path(path, poppler_path=POPPLER_PATH)
                except:
                    st.error("PDF ë³€í™˜ ì‹¤íŒ¨"); continue
                
                # 2. Vision Loop
                page_results = []
                prog_bar = st.progress(0)
                
                for i, p in enumerate(pages):
                    prog_bar.progress((i+1)/len(pages), text=f"ğŸ” Page {i+1} ê²°í•¨ íƒì§€ ì¤‘...")
                    
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as img:
                        p.save(img.name, "JPEG")
                        with open(img.name, "rb") as r:
                            b64 = base64.b64encode(r.read()).decode("utf-8")
                    
                    st.session_state.current_image_base64 = b64 # ë§ˆì§€ë§‰ í˜ì´ì§€ ì €ì¥
                    
                    # ë¶„ì„
                    res = analyze_page_detail(b64, "ì‹œê³µ í’ˆì§ˆ ë° ì•ˆì „ ìœ„í—˜ ìš”ì†Œ")
                    page_results.append({"page": i+1, "content": res})
                    time.sleep(0.1)

                # 3. ì¢…í•© ë³´ê³ ì„œ (+ë¶€ë¡ ê°•ì œ ë³‘í•©)
                status.write("ğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
                final_report = generate_consolidated_report(f.name, page_results)
                
                st.session_state.processed_files.add(f.name)
                st.session_state.messages.append({"role": "assistant", "content": final_report})
                
                prog_bar.empty()
                status.update(label="ë¶„ì„ ì™„ë£Œ", state="complete")

# --- ë©”ì¸ ì±„íŒ…ì°½ ---
st.title("ğŸ—ï¸ ê±´ì„¤ CM AI íŒŒíŠ¸ë„ˆ")

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): st.markdown(msg["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)

    with st.chat_message("assistant"):
        response = ""
        
        # [ë¡œì§ ë¶„ê¸°]
        # 1. Vision Mode (ë„ë©´ ê´€ë ¨ ì§ˆë¬¸ ì„ íƒ ì‹œ)
        if search_mode == "ğŸ“‚ ë„ë©´ ê´€ë ¨ ì§ˆë¬¸" and st.session_state.current_image_base64:
            with st.spinner("ğŸ‘ï¸ ë„ë©´ì„ ê²€í† í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                msg = HumanMessage(content=[
                    {"type": "text", "text": f"ì§ˆë¬¸: {prompt}\n(ì´ì „ ë§¥ë½ê³¼ ë„ë©´ì„ ì°¸ê³ í•˜ì—¬ ê¸°ìˆ ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”)"},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{st.session_state.current_image_base64}"}}
                ])
                response = llm_vision.invoke([msg]).content

        # 2. Deep RAG Mode (ê·¸ ì™¸ ëª¨ë“  ê²½ìš°: ë„ë©´ ì—†ì„ ë•Œ or ë²•ê·œ ê²€ìƒ‰ ëª¨ë“œ)
        else:
            # UI ìƒ ë©”ì‹œì§€ í‘œì‹œ (ì‹¬ì¸µ ê²€ìƒ‰ ì¤‘ì„ì„ ëª…ì‹œ)
            with st.status("ğŸ§  ì‹¬ì¸µ ê²€ìƒ‰ ì¤‘ (Query Expansion + Rerank)...", expanded=True):
                context_docs = retrieve_docs_deep(prompt)
                context_text = format_docs(context_docs)
                response = answer_prompt.pipe(llm_text).pipe(StrOutputParser()).invoke({"context": context_text, "question": prompt})

        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})