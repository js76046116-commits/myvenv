import streamlit as st
import os
import json
import itertools
import base64
import tempfile
import platform 
import time

# [í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬]
from pdf2image import convert_from_path
from sentence_transformers import CrossEncoder 
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory
from langchain_community.retrievers import BM25Retriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage

# ==========================================================
# [0] ê¸°ë³¸ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ==========================================================
st.set_page_config(page_title="ê±´ì„¤ CM AI í†µí•© ì†”ë£¨ì…˜ (ì¢…í•©ë³´ê³ ì„œ)", page_icon="ğŸ—ï¸", layout="wide")

# 1. API í‚¤ ê°€ì ¸ì˜¤ê¸°
if "GOOGLE_API_KEY" in st.secrets:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
elif "GOOGLE_API_KEY" in os.environ:
    GOOGLE_API_KEY = os.environ["GOOGLE_API_KEY"]
else:
    st.error("ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: Google API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

# 2. Poppler ê²½ë¡œ
system_name = platform.system()
if system_name == "Windows":
    POPPLER_PATH = r"C:\Users\owner\myvenv\Release-25.12.0-0\poppler-25.12.0\Library\bin"
else:
    POPPLER_PATH = None 

# 3. ë°ì´í„° ê²½ë¡œ
DB_PATH_1 = "./chroma_db_part1"
DB_PATH_2 = "./chroma_db_part2"
JSON_DATA_PATH = "./legal_data_total_vlm.json"
RAW_DATA = []

# 4. ëª¨ë¸ ì„¤ì •
MODEL_NAME = "models/gemini-2.5-flash" 

# ==========================================================
# [1] ì‹œìŠ¤í…œ ë¡œë”©
# ==========================================================
class SimpleHybridRetriever:
    def __init__(self, bm25, chroma1, chroma2, raw_data):
        self.bm25 = bm25
        self.chroma1 = chroma1
        self.chroma2 = chroma2
        self.raw_data = raw_data
        
    def invoke(self, query):
        docs_bm25 = self.bm25.invoke(query)
        docs_c1 = self.chroma1.invoke(query)
        docs_c2 = self.chroma2.invoke(query)
        
        real_docs_chroma = []
        for doc in (docs_c1 + docs_c2):
            try:
                idx = int(doc.page_content) 
                original_item = self.raw_data[idx] 
                content = original_item.get('content', '').strip()
                source = original_item.get('source', '').strip()
                article = original_item.get('article', '').strip()
                full_text = f"[{source}] {content}"
                new_doc = Document(page_content=full_text, metadata={"source": source, "article": article})
                real_docs_chroma.append(new_doc)
            except:
                continue

        combined = []
        seen_ids = set()
        for d in itertools.chain(docs_bm25, real_docs_chroma):
            key = d.page_content[:30]
            if key not in seen_ids:
                combined.append(d)
                seen_ids.add(key)
        return combined[:200]

@st.cache_resource
def load_search_system():
    global RAW_DATA
    if not os.path.exists(JSON_DATA_PATH):
        st.error("âŒ JSON ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
        
    with open(JSON_DATA_PATH, 'r', encoding='utf-8') as f:
        RAW_DATA = json.load(f)

    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=GOOGLE_API_KEY)
    
    if not os.path.exists(DB_PATH_1) or not os.path.exists(DB_PATH_2):
        st.error("âŒ DB í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    store1 = Chroma(persist_directory=DB_PATH_1, embedding_function=embeddings, collection_name="construction_laws")
    retriever1 = store1.as_retriever(search_kwargs={"k": 100})
    store2 = Chroma(persist_directory=DB_PATH_2, embedding_function=embeddings, collection_name="construction_laws")
    retriever2 = store2.as_retriever(search_kwargs={"k": 100})

    docs = []
    for item in RAW_DATA:
        content = item.get('content', '').strip()
        source = item.get('source', '').strip()
        if not content: continue
        doc = Document(page_content=f"[{source}] {content}", metadata={"source": source, "article": item.get('article', '')})
        docs.append(doc)
    
    bm25_retriever = BM25Retriever.from_documents(docs)
    bm25_retriever.k = 150
    hybrid_retriever = SimpleHybridRetriever(bm25_retriever, retriever1, retriever2, RAW_DATA)
    reranker = CrossEncoder("cross-encoder/ms-marco-TinyBERT-L-2-v2", model_kwargs={"dtype": "auto"})

    return hybrid_retriever, reranker

with st.spinner("ğŸš€ AI ì—”ì§„ ì‹œë™ ì¤‘..."):
    try:
        hybrid_retriever, reranker_model = load_search_system()
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ë¡œë”© ì‹¤íŒ¨: {e}")
        st.stop()

# ì•ˆì „ ì„¤ì • (ê±´ì„¤ í˜„ì¥ ì‚¬ì§„ ì°¨ë‹¨ ë°©ì§€)
safety_settings = {
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

llm_text = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0, google_api_key=GOOGLE_API_KEY, safety_settings=safety_settings)
llm_vision = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0, google_api_key=GOOGLE_API_KEY, safety_settings=safety_settings)

# ==========================================================
# [2] ë¶„ì„ ë¡œì§ (í˜ì´ì§€ ë¶„ì„ -> ì¢…í•© ë¦¬í¬íŠ¸)
# ==========================================================
spacing_chain = ChatPromptTemplate.from_template("êµì •ëœ í•œêµ­ì–´ ë¬¸ì¥ë§Œ ì¶œë ¥(ì„¤ëª…X): {question}").pipe(llm_text).pipe(StrOutputParser())

def retrieve_and_rerank(query, top_k=5):
    initial_docs = hybrid_retriever.invoke(query)
    if not initial_docs: return []
    pairs = [[query, doc.page_content] for doc in initial_docs]
    scores = []
    batch_size = 16
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i : i + batch_size]
        batch_scores = reranker_model.predict(batch)
        scores.extend(batch_scores)
    scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in scored_docs[:top_k]]

# [A] í˜ì´ì§€ë³„ ì •ë°€ ì§„ë‹¨ (Vision)
def analyze_page_detail(image_base64, query, retrieved_docs):
    laws_text = "\n".join([f"- {d.page_content}" for d in retrieved_docs])
    if not laws_text.strip():
        laws_text = "(ê²€ìƒ‰ëœ ê´€ë ¨ ì‹œë°©ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì‹œê³µ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.)"

    prompt_text = f"""
    ë‹¹ì‹ ì€ ê±´ì„¤ í˜„ì¥ì˜ **ì‹œê³µ í’ˆì§ˆ ë° ì•ˆì „ ê´€ë¦¬ ì „ë¬¸ê°€(Construction CM Expert)**ì…ë‹ˆë‹¤.
    
    [ê²€í†  ìš”ì²­] {query}
    [ì°¸ê³  ê¸°ì¤€] {laws_text}
    
    [ì§€ì‹œì‚¬í•­]
    1. ë„ë©´ì„ ì •ë°€í•˜ê²Œ ë³´ê³  ì‹œê³µ ì‹œ ë°œìƒ ê°€ëŠ¥í•œ **í’ˆì§ˆ ë¬¸ì œ(ê· ì—´, ëˆ„ìˆ˜, ê²°ë¡œ)**ì™€ **ì•ˆì „ ìœ„í—˜(ì¶”ë½, ì „ë„)**ì„ ì°¾ì•„ë‚´ì„¸ìš”.
    2. ë°˜ë“œì‹œ ìœ„ [ì°¸ê³  ê¸°ì¤€]ì˜ ì‹œë°©ì„œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ì§€ì í•˜ì„¸ìš”.
    3. **ë©”ëª¨ í˜•ì‹**ìœ¼ë¡œ í•µì‹¬ë§Œ ê°„ë‹¨íˆ ì‘ì„±í•˜ì„¸ìš”. (ë‚˜ì¤‘ì— ì¢…í•©í•  ê²ƒì…ë‹ˆë‹¤.)
    """
    
    message = HumanMessage(content=[
        {"type": "text", "text": prompt_text},
        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}},
    ])
    
    try:
        response = llm_vision.invoke([message])
        return response.content if response.content else "íŠ¹ì´ì‚¬í•­ ì—†ìŒ."
    except Exception as e:
        return f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

# [B] ì¢…í•© ë³´ê³ ì„œ ì‘ì„± (Text summarization)
def generate_final_report(file_name, page_results):
    # í˜ì´ì§€ë³„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
    raw_data = ""
    for item in page_results:
        raw_data += f"\n[Page {item['page']} ì§„ë‹¨ë‚´ìš©]:\n{item['content']}\n"
    
    prompt = f"""
    ë‹¹ì‹ ì€ ê±´ì„¤ì‚¬ì—…ê´€ë¦¬ë‹¨ì¥(CMë‹¨ì¥)ì…ë‹ˆë‹¤.
    ê° í˜ì´ì§€ë³„ ë‹´ë‹¹ìê°€ ë³´ê³ í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **'{file_name}'ì— ëŒ€í•œ ìµœì¢… ì‹œê³µ í’ˆì§ˆ/ì•ˆì „ ê²€í†  ë³´ê³ ì„œ**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

    [ë‹´ë‹¹ì ë³´ê³  ë‚´ìš© í•©ë³¸]
    {raw_data}

    [ë³´ê³ ì„œ ì‘ì„± ê·œì¹™]
    1. **ì¤‘ë³µ í†µí•©:** ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ë°˜ë³µë˜ëŠ” ì§€ì  ì‚¬í•­ì€ í•˜ë‚˜ë¡œ í•©ì³ì„œ ê°•ë ¥í•˜ê²Œ ê¶Œê³ í•˜ì„¸ìš”.
    2. **êµ¬ì¡°í™”ëœ ëª©ì°¨:**
       # ğŸ—ï¸ [ì¢…í•©] ì‹œê³µ í’ˆì§ˆ ë° ì•ˆì „ ê²€í†  ë³´ê³ ì„œ
       ## 1. ì´í‰ (Executive Summary)
       ## 2. ì£¼ìš” ì‹œê³µ ê´€ë¦¬ í¬ì¸íŠ¸ (LH ì‹œë°©ì„œ ê¸°ì¤€)
          - í’ˆì§ˆ ê´€ë¦¬ (ê· ì—´, ë°©ìˆ˜, ë‹¨ì—´ ë“±)
          - ì•ˆì „ ê´€ë¦¬ (ì¶”ë½, ë‚™í•˜, ì¥ë¹„ ë“±)
       ## 3. í˜ì´ì§€ë³„ íŠ¹ì´ì‚¬í•­ (Issues by Page)
          - (ë¬¸ì œê°€ ë°œê²¬ëœ í˜ì´ì§€ë§Œ ìš”ì•½í•˜ì—¬ ê¸°ì¬)
    3. **í†¤ì•¤ë§¤ë„ˆ:** ì „ë¬¸ì ì´ê³  ë‹¨í˜¸í•œ ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    return llm_text.invoke(prompt).content

answer_prompt = ChatPromptTemplate.from_messages([
    ("system", "ê±´ì„¤ ê¸°ì¤€ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. [Context]ë¥¼ ë³´ê³  ë‹µë³€í•˜ì„¸ìš”.\n[Context]\n{context}"),
    ("human", "ì§ˆë¬¸: {question}")
])

def format_docs(docs):
    return "\n\n".join([f"<ì¶œì²˜: {d.metadata.get('source')} / {d.metadata.get('article')}>\n{d.page_content}" for d in docs])

rag_chain = (
    {"context": RunnableLambda(lambda x: retrieve_and_rerank(x, top_k=10)) | format_docs, "question": RunnablePassthrough()}
    | answer_prompt | llm_text | StrOutputParser()
)

# ==========================================================
# [3] ì›¹ UI êµ¬ì„±
# ==========================================================
st.title("ğŸ—ï¸ ê±´ì„¤ CM ì „ë¬¸ AI (ì‹œê³µ í’ˆì§ˆ/ì•ˆì „ ì¢…í•©ë¶„ì„)")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processed_files" not in st.session_state:
    st.session_state.processed_files = set() # ì²˜ë¦¬í•œ íŒŒì¼ ê¸°ì–µì¥ì†Œ
if "current_image_base64" not in st.session_state:
    st.session_state.current_image_base64 = None

# --- [A] ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ğŸ“‚ ë„ë©´ íˆ¬ì…êµ¬")
    st.info("ğŸ’¡ PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ **ì „ì²´ í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ì—¬ í•˜ë‚˜ì˜ ì¢…í•© ë³´ê³ ì„œ**ë¥¼ ë§Œë“­ë‹ˆë‹¤.")
    uploaded_files = st.file_uploader("ê²€í† í•  ë„ë©´ PDF", type=["pdf"], accept_multiple_files=True)

# --- [B] ìë™ ë¶„ì„ ë¡œì§ (ìˆœì°¨ ì²˜ë¦¬ + ì¢…í•©) ---
if uploaded_files:
    for target_file in uploaded_files:
        # ì´ë¯¸ ì²˜ë¦¬í•œ íŒŒì¼ì€ ê±´ë„ˆëœ€ (ì¤‘ë³µ ë¶„ì„ ë°©ì§€)
        if target_file.name not in st.session_state.processed_files:
            
            # 1. íŒŒì¼ ë³€í™˜ ì•Œë¦¼
            with st.status(f"ğŸ“„ '{target_file.name}' ë„ë©´ ìŠ¤ìº” ì¤‘...", expanded=True) as status:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(target_file.read())
                    tmp_path = tmp_file.name
                
                try:
                    all_pages = convert_from_path(tmp_path, poppler_path=POPPLER_PATH)
                    status.write(f"âœ… ì´ {len(all_pages)}í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ. ì •ë°€ ì§„ë‹¨ ì‹œì‘...")
                except Exception as e:
                    st.error(f"ë³€í™˜ ì‹¤íŒ¨: {e}")
                    continue

                # 2. í˜ì´ì§€ë³„ ë£¨í”„ (Vision Analysis)
                page_results = []
                progress_bar = st.progress(0)
                
                for i, page_img in enumerate(all_pages):
                    page_num = i + 1
                    progress_text = f"ğŸ” Page {page_num}/{len(all_pages)} ì •ë°€ ë¶„ì„ ì¤‘... (ì‹œë°©ì„œ ëŒ€ì¡°)"
                    progress_bar.progress((i + 1) / len(all_pages), text=progress_text)
                    
                    # ì´ë¯¸ì§€ ì¸ì½”ë”©
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp_img:
                        page_img.save(tmp_img.name, "JPEG")
                        with open(tmp_img.name, "rb") as f:
                            img_base64 = base64.b64encode(f.read()).decode("utf-8")
                    
                    # ê°€ì¥ ìµœê·¼ ë³¸ ì´ë¯¸ì§€ ì €ì¥ (ì¶”ê°€ ì§ˆë¬¸ìš©)
                    st.session_state.current_image_base64 = img_base64
                    
                    # ê°œë³„ í˜ì´ì§€ ë¶„ì„
                    query = "ì´ ë„ë©´ì˜ ì‹œê³µ í’ˆì§ˆ ë° ì•ˆì „ ìœ„í—˜ ìš”ì†Œë¥¼ ì°¾ì•„ì¤˜."
                    retrieved_docs = retrieve_and_rerank(query, top_k=3)
                    result = analyze_page_detail(img_base64, query, retrieved_docs)
                    
                    # ê²°ê³¼ ë©”ëª¨
                    page_results.append({"page": page_num, "content": result})

                # 3. ì¢…í•© ë³´ê³ ì„œ ì‘ì„± (Consolidation)
                status.write("ğŸ“ í˜ì´ì§€ë³„ ì§„ë‹¨ ì™„ë£Œ. ì¢…í•© ë³´ê³ ì„œ ì‘ì„± ì¤‘...")
                final_report = generate_final_report(target_file.name, page_results)
                
                # 4. ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
                st.session_state.processed_files.add(target_file.name)
                st.session_state.messages.append({"role": "assistant", "content": final_report})
                
                progress_bar.empty()
                status.update(label=f"âœ… '{target_file.name}' ë¶„ì„ ì™„ë£Œ!", state="complete")

# --- [C] ì±„íŒ…ì°½ í‘œì‹œ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- [D] ì‚¬ìš©ì ì§ˆë¬¸ ---
if prompt := st.chat_input("ë³´ê³ ì„œ ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆë‚˜ìš”?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ (ë°©ê¸ˆ ë¶„ì„í•œ ë„ë©´ ê¸°ì¤€)
        if st.session_state.current_image_base64:
            with st.status("ğŸ” ë„ë©´ ì¬ê²€í†  ë° ë‹µë³€ ì¤‘...", expanded=True):
                retrieved_docs = retrieve_and_rerank(prompt, top_k=5)
                # Vision AIì—ê²Œ ë‹¤ì‹œ ë¬¼ì–´ë´„
                prompt_text = f"ì‚¬ìš©ì ì§ˆë¬¸: {prompt}\n(ì´ì „ ë¶„ì„ ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.)"
                message = HumanMessage(content=[
                    {"type": "text", "text": prompt_text},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{st.session_state.current_image_base64}"}},
                ])
                response = llm_vision.invoke([message]).content
            
            # ê·¼ê±° ìë£Œ í‘œì‹œ
            refs = "\n\n[ê´€ë ¨ ê·¼ê±°]: " + ", ".join([d.metadata.get('article', 'ì¶œì²˜ë¯¸ìƒ') for d in retrieved_docs])
            final_res = response + refs
            st.markdown(final_res)
            st.session_state.messages.append({"role": "assistant", "content": final_res})
        
        # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ (ì¼ë°˜ í…ìŠ¤íŠ¸ ì§ˆë¬¸)
        else:
            corrected = spacing_chain.invoke({"question": prompt})
            response = rag_chain.invoke(corrected)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})