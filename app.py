import streamlit as st
import os
import json
import itertools
import base64
import tempfile
import platform  # [í•µì‹¬] ìš´ì˜ì²´ì œ ê°ì§€ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
from pdf2image import convert_from_path
from sentence_transformers import CrossEncoder 

from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.retrievers import BM25Retriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage

# ==========================================================
# [0] ê¸°ë³¸ ì„¤ì • (ê²½ë¡œ ë° API)
# ==========================================================
st.set_page_config(page_title="ê±´ì„¤ CM AI í†µí•© ì†”ë£¨ì…˜", page_icon="ğŸ—ï¸", layout="wide")

# 1. API í‚¤ (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)
if "GOOGLE_API_KEY" not in os.environ:
    # os.environ["GOOGLE_API_KEY"] = "ì—¬ê¸°ì—_í‚¤ë¥¼_ë„£ìœ¼ì„¸ìš”" # Streamlit Secretsë¥¼ ì“´ë‹¤ë©´ ì£¼ì„ ìœ ì§€
    pass

# 2. Poppler ê²½ë¡œ ì„¤ì • (ìë™ ê°ì§€ ë¡œì§)
# [ì¤‘ìš”] ìœˆë„ìš°ì™€ ë¦¬ëˆ…ìŠ¤(ì„œë²„)ë¥¼ êµ¬ë¶„í•˜ì—¬ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
system_name = platform.system()

if system_name == "Windows":
    # ì‚¬ìš©ì ë¡œì»¬ ì»´í“¨í„°ìš© ê²½ë¡œ
    POPPLER_PATH = r"C:\Users\owner\myvenv\Release-25.12.0-0\poppler-25.12.0\Library\bin"
else:
    # Streamlit Cloud (Linux) ì„œë²„ìš© 
    # packages.txtë¥¼ í†µí•´ ì„¤ì¹˜ëœ poppler-utilsëŠ” ì‹œìŠ¤í…œ PATHì— ë“±ë¡ë˜ë¯€ë¡œ ê²½ë¡œ ì§€ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤(None).
    POPPLER_PATH = None 

# 3. ë°ì´í„° ê²½ë¡œ
DB_PATH_1 = "./chroma_db_part1"
DB_PATH_2 = "./chroma_db_part2"
JSON_DATA_PATH = "./legal_data_total_vlm.json"

# ì „ì—­ ë³€ìˆ˜
RAW_DATA = []

# ==========================================================
# [1] ì‹œìŠ¤í…œ ë¡œë”© (DB + Hybrid Search + Vision)
# ==========================================================
class SimpleHybridRetriever:
    def __init__(self, bm25, chroma1, chroma2, raw_data):
        self.bm25 = bm25
        self.chroma1 = chroma1
        self.chroma2 = chroma2
        self.raw_data = raw_data
        
    def invoke(self, query):
        # 1. BM25 & Chroma ê²€ìƒ‰
        docs_bm25 = self.bm25.invoke(query)
        docs_c1 = self.chroma1.invoke(query)
        docs_c2 = self.chroma2.invoke(query)
        
        # 2. ID -> ì›ë³¸ í…ìŠ¤íŠ¸ ë³µì›
        real_docs_chroma = []
        for doc in (docs_c1 + docs_c2):
            try:
                idx = int(doc.page_content) 
                original_item = self.raw_data[idx] 
                
                content = original_item.get('content', '').strip()
                source = original_item.get('source', '').strip()
                article = original_item.get('article', '').strip()
                full_text = f"[{source}] {content}"
                
                new_doc = Document(page_content=full_text, metadata={"source": source, "article": article})
                real_docs_chroma.append(new_doc)
            except:
                continue

        # 3. ì¤‘ë³µ ì œê±°
        combined = []
        seen_ids = set()
        for d in itertools.chain(docs_bm25, real_docs_chroma):
            key = d.page_content[:30]
            if key not in seen_ids:
                combined.append(d)
                seen_ids.add(key)
        return combined[:200]

@st.cache_resource
def load_search_system():
    global RAW_DATA
    
    # JSON ë¡œë“œ
    if not os.path.exists(JSON_DATA_PATH):
        st.error("âŒ JSON ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    with open(JSON_DATA_PATH, 'r', encoding='utf-8') as f:
        RAW_DATA = json.load(f)

    # ì„ë² ë”© & DB ë¡œë“œ
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    if not os.path.exists(DB_PATH_1) or not os.path.exists(DB_PATH_2):
        st.error("âŒ DB í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    store1 = Chroma(persist_directory=DB_PATH_1, embedding_function=embeddings, collection_name="construction_laws")
    retriever1 = store1.as_retriever(search_kwargs={"k": 100})
    store2 = Chroma(persist_directory=DB_PATH_2, embedding_function=embeddings, collection_name="construction_laws")
    retriever2 = store2.as_retriever(search_kwargs={"k": 100})

    # BM25 ìƒì„±
    docs = []
    for item in RAW_DATA:
        content = item.get('content', '').strip()
        source = item.get('source', '').strip()
        if not content: continue
        doc = Document(page_content=f"[{source}] {content}", metadata={"source": source, "article": item.get('article', '')})
        docs.append(doc)
    
    bm25_retriever = BM25Retriever.from_documents(docs)
    bm25_retriever.k = 150

    hybrid_retriever = SimpleHybridRetriever(bm25_retriever, retriever1, retriever2, RAW_DATA)
    reranker = CrossEncoder("cross-encoder/ms-marco-TinyBERT-L-2-v2", model_kwargs={"torch_dtype": "auto"})

    return hybrid_retriever, reranker

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
with st.spinner("ğŸš€ AI í†µí•© ì—”ì§„(Text+Vision) ì‹œë™ ì¤‘..."):
    hybrid_retriever, reranker_model = load_search_system()

# ëª¨ë¸ ì„¤ì •
llm_text = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)
llm_vision = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0) # ë¹„ì „ íŠ¹í™” ëª¨ë¸

# ==========================================================
# [2] ë¡œì§ ì²´ì¸ (Logic Chains)
# ==========================================================

# 1. í…ìŠ¤íŠ¸ êµì • ë° í™•ì¥ ì²´ì¸
spacing_chain = ChatPromptTemplate.from_template("êµì •ëœ í•œêµ­ì–´ ë¬¸ì¥ë§Œ ì¶œë ¥(ì„¤ëª…X): {question}").pipe(llm_text).pipe(StrOutputParser())
hyde_chain = ChatPromptTemplate.from_template("ê±´ì„¤ ì „ë¬¸ ê²€ìƒ‰ í‚¤ì›Œë“œ 5ê°œ ë‚˜ì—´(ì½¤ë§ˆ êµ¬ë¶„, ì„¤ëª…X): {question}").pipe(llm_text).pipe(StrOutputParser())

# 2. ê²€ìƒ‰ ë° ë¦¬ë­í‚¹ í•¨ìˆ˜
def retrieve_and_rerank(query, top_k=5):
    initial_docs = hybrid_retriever.invoke(query)
    if not initial_docs: return []
    
    pairs = [[query, doc.page_content] for doc in initial_docs]
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ
    scores = []
    batch_size = 16
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i : i + batch_size]
        batch_scores = reranker_model.predict(batch)
        scores.extend(batch_scores)
        
    scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in scored_docs[:top_k]]

# 3. Vision ë¶„ì„ í•¨ìˆ˜ (PDF ì‹¬ì¸µ ë¶„ì„ìš©)
def analyze_drawing_deep(image_base64, query, retrieved_docs):
    laws_text = "\n".join([f"- {d.page_content}" for d in retrieved_docs])
    
    prompt_text = f"""
    ë‹¹ì‹ ì€ ê±´ì¶• ë„ë©´ ê²€í†  ë° ë²•ê·œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
    [ì‚¬ìš©ì ì§ˆë¬¸]
    {query}
    
    [ê´€ë ¨ ë²•ê·œ ë°ì´í„°ë² ì´ìŠ¤]
    {laws_text}
    
    [ë¶„ì„ ì§€ì‹œì‚¬í•­]
    1. **ë²•ê·œ ë§¤í•‘:** ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë²•ê·œë¥¼ ìœ„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì•„ë‚´ì„¸ìš”.
    2. **ë„ë©´ ì¸ì‹:** ì²¨ë¶€ëœ ë„ë©´ ì´ë¯¸ì§€ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬ ë²½ì²´, ê³µê°„, ì¹˜ìˆ˜, ì‹¬ë³¼ì„ ì‹ë³„í•˜ì„¸ìš”.
    3. **ìœ„ë°˜ ê²€í† :** ë„ë©´ì˜ ë‚´ìš©ì´ ë²•ê·œ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ”ì§€ ì—„ê²©í•˜ê²Œ ëŒ€ì¡°í•˜ì„¸ìš”.
    4. **ê²°ê³¼ ì¶œë ¥:** - ìœ„ë°˜ ì—¬ë¶€ (ì í•©/ë¶€ì í•©/íŒë‹¨ë¶ˆê°€)
       - êµ¬ì²´ì ì¸ ê·¼ê±° (ë„ë©´ì˜ ì–´ëŠ ë¶€ë¶„, ì–´ë–¤ ì¹˜ìˆ˜ ë•Œë¬¸ì¸ì§€)
       - ê°œì„  ì œì•ˆ (í•„ìš”ì‹œ)
    """
    
    message = HumanMessage(
        content=[
            {"type": "text", "text": prompt_text},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}},
        ]
    )
    return llm_vision.invoke([message]).content

# 4. ì¼ë°˜ í…ìŠ¤íŠ¸ ë‹µë³€ ì²´ì¸
answer_prompt = ChatPromptTemplate.from_messages([
    ("system", "ê±´ì„¤ ê¸°ì¤€ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. [Context]ë¥¼ ë³´ê³  ë‹µë³€í•˜ì„¸ìš”. ì¶œì²˜ í‘œê¸° í•„ìˆ˜.\n[Context]\n{context}"),
    ("human", "ì§ˆë¬¸: {question}")
])
def format_docs(docs):
    return "\n\n".join([f"<ì¶œì²˜: {d.metadata.get('source')} / {d.metadata.get('article')}>\n{d.page_content}" for d in docs])

rag_chain = (
    {"context": RunnableLambda(lambda x: retrieve_and_rerank(x, top_k=20)) | format_docs, "question": RunnablePassthrough()}
    | answer_prompt | llm_text | StrOutputParser()
)

# ==========================================================
# [3] ì›¹ UI êµ¬ì„±
# ==========================================================
st.title("ğŸ—ï¸ ê±´ì„¤ CM ì „ë¬¸ AI (ë„ë©´ + ë²•ê·œ)")

# --- [A] ì‚¬ì´ë“œë°”: PDF í´ë”/íŒŒì¼ ì„ íƒ ì°½ ---
with st.sidebar:
    st.header("ğŸ“‚ ë„ë©´ íˆ¬ì…êµ¬ (PDF)")
    st.info("ğŸ’¡ PDFë¥¼ ë„£ìœ¼ë©´ ìë™ìœ¼ë¡œ 'ì‹¬ì¸µ ë„ë©´ ë¶„ì„' ëª¨ë“œë¡œ ì „í™˜ë©ë‹ˆë‹¤.")
    
    # íŒŒì¼ ì—…ë¡œë” (ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥, í´ë” ì„ íƒê³¼ ìœ ì‚¬í•œ íš¨ê³¼)
    uploaded_files = st.file_uploader("ê²€í† í•  ë„ë©´ PDFë¥¼ ì„ íƒí•˜ì„¸ìš”", type=["pdf"], accept_multiple_files=True)
    
    current_image_base64 = None
    
    if uploaded_files:
        # í¸ì˜ìƒ ì²« ë²ˆì§¸ íŒŒì¼ë§Œ ì²˜ë¦¬ (ì¶”í›„ ë¦¬ìŠ¤íŠ¸ë¡œ í™•ì¥ ê°€ëŠ¥)
        target_file = uploaded_files[0]
        st.write(f"ğŸ“„ ì„ íƒëœ íŒŒì¼: **{target_file.name}**")
        
        with st.spinner("ì´ë¯¸ì§€ ë³€í™˜ ì¤‘..."):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(target_file.read())
                tmp_path = tmp_file.name
            
            try:
                # 1í˜ì´ì§€ë§Œ ë³€í™˜ (ì†ë„ ìµœì í™”)
                # [ì¤‘ìš”] ì—¬ê¸°ì„œ ìœ„ì—ì„œ ì„¤ì •í•œ POPPLER_PATH ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                images = convert_from_path(tmp_path, poppler_path=POPPLER_PATH, first_page=1, last_page=1)
                if images:
                    st.image(images[0], caption="ê²€í†  ëŒ€ìƒ ë„ë©´", use_container_width=True)
                    
                    # Base64 ë³€í™˜
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp_img:
                        images[0].save(tmp_img.name, "JPEG")
                        with open(tmp_img.name, "rb") as f:
                            current_image_base64 = base64.b64encode(f.read()).decode("utf-8")
                else:
                    st.error("ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")

# --- [B] ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- [C] ì§ˆë¬¸ ì²˜ë¦¬ ë° ë¶„ê¸° ë¡œì§ ---
if prompt := st.chat_input("ì§ˆë¬¸ ë˜ëŠ” ë„ë©´ ê²€í†  ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        
        # [Case 1: PDFê°€ ìˆëŠ” ê²½ìš° -> ë¬´ì¡°ê±´ ì‹¬ì¸µ ë¶„ì„ (HyDE + Vision)]
        if current_image_base64:
            with st.status("ğŸ§  ë„ë©´ ì‹¬ì¸µ ë¶„ì„ ëª¨ë“œ ê°€ë™...", expanded=True) as status:
                # 1. HyDEë¡œ ê²€ìƒ‰ í‚¤ì›Œë“œ í™•ì¥
                st.write("ğŸ”§ ì§ˆë¬¸ ì˜ë„ íŒŒì•… ë° ë²•ê·œ ê²€ìƒ‰ì–´ í™•ì¥ ì¤‘...")
                hyde_keywords = hyde_chain.invoke({"question": prompt})
                expanded_query = f"{prompt} {hyde_keywords}"
                
                # 2. ê´€ë ¨ ë²•ê·œ ê²€ìƒ‰ (RAG)
                st.write("ğŸ“š ê´€ë ¨ ê±´ì„¤ ë²•ê·œ/ê¸°ì¤€ ì •ë°€ ê²€ìƒ‰ ì¤‘...")
                retrieved_docs = retrieve_and_rerank(expanded_query, top_k=5)
                
                # 3. Vision AI ë¶„ì„
                st.write("ğŸ‘€ ë„ë©´ ì‹œê°ì  ë¶„ì„ ë° ë²•ê·œ ëŒ€ì¡° ì¤‘...")
                vision_response = analyze_drawing_deep(current_image_base64, prompt, retrieved_docs)
                
                status.update(label="âœ… ì‹¬ì¸µ ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)
            
            # ê²°ê³¼ ì¶œë ¥
            final_res = f"### ğŸ—ï¸ ë„ë©´ ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼\n\n{vision_response}\n\n---\n**[ì°¸ê³ í•œ ë²•ê·œ ë° í‚¤ì›Œë“œ]**\n- í™•ì¥ í‚¤ì›Œë“œ: `{hyde_keywords}`\n" + "\n".join([f"- {d.metadata.get('source')} ({d.metadata.get('article')})" for d in retrieved_docs])
            st.markdown(final_res)
            st.session_state.messages.append({"role": "assistant", "content": final_res})
            
        # [Case 2: PDFê°€ ì—†ëŠ” ê²½ìš° -> ê¸°ì¡´ í…ìŠ¤íŠ¸ ëª¨ë“œ (1ì°¨ -> 2ì°¨)]
        else:
            with st.status("ğŸ” 1ì°¨ ê²€ìƒ‰ ì§„í–‰ ì¤‘...", expanded=True) as status:
                corrected_query = spacing_chain.invoke({"question": prompt})
                response_1 = rag_chain.invoke(corrected_query)
                status.update(label="âœ… 1ì°¨ ê²€ìƒ‰ ì™„ë£Œ", state="complete", expanded=False)
            
            msg_content = f"### ğŸ¤– 1ì°¨ ë‹µë³€\n{response_1}"
            st.markdown(msg_content)
            st.session_state.messages.append({"role": "assistant", "content": msg_content})
            st.rerun() # ë²„íŠ¼ í‘œì‹œë¥¼ ìœ„í•´ ë¦¬ëŸ°

# --- [D] í…ìŠ¤íŠ¸ ëª¨ë“œì¼ ë•Œë§Œ ì‹¬ì¸µ ê²€ìƒ‰ ë²„íŠ¼ í‘œì‹œ ---
last_msg = st.session_state.messages[-1] if st.session_state.messages else None
if last_msg and last_msg["role"] == "assistant" and "1ì°¨ ë‹µë³€" in last_msg["content"] and "2ì°¨" not in last_msg["content"]:
    with st.expander("ğŸ¤” ë‹µë³€ì´ ë¶€ì¡±í•œê°€ìš”? (HyDE ì‹¬ì¸µ ê²€ìƒ‰)"):
        if st.button("ğŸš€ ì‹¬ì¸µ ê²€ìƒ‰ ì‹¤í–‰"):
            prev_question = st.session_state.messages[-2]["content"]
            
            with st.status("ğŸ§  ì „ë¬¸ê°€ ëª¨ë“œ(HyDE) ê°€ë™ ì¤‘...", expanded=True) as status:
                hyde_keywords = hyde_chain.invoke({"question": prev_question})
                final_query = f"{prev_question} {hyde_keywords}"
                response_2 = rag_chain.invoke(final_query)
                status.update(label="âœ… ì™„ë£Œ", state="complete")
            
            final_res = f"### ğŸ¤– 2ì°¨ ìƒì„¸ ë‹µë³€ (HyDE)\n**í™•ì¥ëœ ê²€ìƒ‰ì–´:** `{hyde_keywords}`\n\n{response_2}"
            st.session_state.messages.append({"role": "assistant", "content": final_res})
            st.rerun()