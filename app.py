import streamlit as st
import os
import json
import itertools
import base64
import tempfile
import platform 
import time

# [í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬]
from pdf2image import convert_from_path
from sentence_transformers import CrossEncoder 
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.retrievers import BM25Retriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage

# ==========================================================
# [0] ê¸°ë³¸ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ==========================================================
st.set_page_config(page_title="ê±´ì„¤ CM AI í†µí•© ì†”ë£¨ì…˜ (ì‹œê³µ/í’ˆì§ˆ)", page_icon="ğŸ—ï¸", layout="wide")

# 1. API í‚¤ ê°€ì ¸ì˜¤ê¸° (Secrets ìš°ì„  -> í™˜ê²½ë³€ìˆ˜)
if "GOOGLE_API_KEY" in st.secrets:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
elif "GOOGLE_API_KEY" in os.environ:
    GOOGLE_API_KEY = os.environ["GOOGLE_API_KEY"]
else:
    st.error("ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: Google API Keyê°€ ì—†ìŠµë‹ˆë‹¤. Streamlit Secrets ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# í™˜ê²½ ë³€ìˆ˜ ë™ê¸°í™”
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

# 2. Poppler ê²½ë¡œ ì„¤ì • (Windows ë¡œì»¬ / ë¦¬ëˆ…ìŠ¤ ì„œë²„ ìë™ êµ¬ë¶„)
system_name = platform.system()
if system_name == "Windows":
    # ì‚¬ìš©ìë‹˜ ë¡œì»¬ ê²½ë¡œ (ìˆ˜ì • ê¸ˆì§€)
    POPPLER_PATH = r"C:\Users\owner\myvenv\Release-25.12.0-0\poppler-25.12.0\Library\bin"
else:
    # Streamlit Cloud ì„œë²„ìš© (ìë™ ì„¤ì¹˜ë¨)
    POPPLER_PATH = None 

# 3. ë°ì´í„° ê²½ë¡œ
DB_PATH_1 = "./chroma_db_part1"
DB_PATH_2 = "./chroma_db_part2"
JSON_DATA_PATH = "./legal_data_total_vlm.json"
RAW_DATA = []

# 4. ëª¨ë¸ ì„¤ì • (ì‚¬ìš©ì ë¡œê·¸ ê¸°ë°˜ ìƒì¡´ ëª¨ë¸)
MODEL_NAME = "models/gemini-2.5-flash" 

# ==========================================================
# [1] ì‹œìŠ¤í…œ ë¡œë”© (ê²€ìƒ‰ì—”ì§„ + AIëª¨ë¸)
# ==========================================================
class SimpleHybridRetriever:
    def __init__(self, bm25, chroma1, chroma2, raw_data):
        self.bm25 = bm25
        self.chroma1 = chroma1
        self.chroma2 = chroma2
        self.raw_data = raw_data
        
    def invoke(self, query):
        # 3ê°€ì§€ ê²€ìƒ‰ê¸° ë™ì‹œ ê°€ë™
        docs_bm25 = self.bm25.invoke(query)
        docs_c1 = self.chroma1.invoke(query)
        docs_c2 = self.chroma2.invoke(query)
        
        # Chroma ê²°ê³¼(ID)ë¥¼ ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        real_docs_chroma = []
        for doc in (docs_c1 + docs_c2):
            try:
                idx = int(doc.page_content) 
                original_item = self.raw_data[idx] 
                content = original_item.get('content', '').strip()
                source = original_item.get('source', '').strip()
                article = original_item.get('article', '').strip()
                full_text = f"[{source}] {content}"
                new_doc = Document(page_content=full_text, metadata={"source": source, "article": article})
                real_docs_chroma.append(new_doc)
            except:
                continue

        # ì¤‘ë³µ ì œê±° ë° ê²°í•©
        combined = []
        seen_ids = set()
        for d in itertools.chain(docs_bm25, real_docs_chroma):
            key = d.page_content[:30] # ë‚´ìš© ì•ë¶€ë¶„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            if key not in seen_ids:
                combined.append(d)
                seen_ids.add(key)
        return combined[:200]

@st.cache_resource
def load_search_system():
    global RAW_DATA
    if not os.path.exists(JSON_DATA_PATH):
        st.error("âŒ JSON ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (legal_data_total_vlm.json)")
        st.stop()
        
    with open(JSON_DATA_PATH, 'r', encoding='utf-8') as f:
        RAW_DATA = json.load(f)

    # ì„ë² ë”© ëª¨ë¸ (API í‚¤ ì§ì ‘ ì£¼ì…)
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004", 
        google_api_key=GOOGLE_API_KEY
    )
    
    if not os.path.exists(DB_PATH_1) or not os.path.exists(DB_PATH_2):
        st.error("âŒ DB í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. (chroma_db_part1, part2)")
        st.stop()

    # Chroma DB ë¡œë“œ
    store1 = Chroma(persist_directory=DB_PATH_1, embedding_function=embeddings, collection_name="construction_laws")
    retriever1 = store1.as_retriever(search_kwargs={"k": 100})
    store2 = Chroma(persist_directory=DB_PATH_2, embedding_function=embeddings, collection_name="construction_laws")
    retriever2 = store2.as_retriever(search_kwargs={"k": 100})

    # BM25(í‚¤ì›Œë“œ ê²€ìƒ‰) ë¡œë“œ
    docs = []
    for item in RAW_DATA:
        content = item.get('content', '').strip()
        source = item.get('source', '').strip()
        if not content: continue
        doc = Document(page_content=f"[{source}] {content}", metadata={"source": source, "article": item.get('article', '')})
        docs.append(doc)
    
    bm25_retriever = BM25Retriever.from_documents(docs)
    bm25_retriever.k = 150

    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ìƒì„±
    hybrid_retriever = SimpleHybridRetriever(bm25_retriever, retriever1, retriever2, RAW_DATA)
    
    # Reranker (ì •í™•ë„ í–¥ìƒ)
    reranker = CrossEncoder(
        "cross-encoder/ms-marco-TinyBERT-L-2-v2", 
        model_kwargs={"dtype": "auto"}
    )

    return hybrid_retriever, reranker

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
with st.spinner("ğŸš€ AI ê±´ì„¤ í†µí•© ì—”ì§„(Text+Vision) ì‹œë™ ì¤‘..."):
    try:
        hybrid_retriever, reranker_model = load_search_system()
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ë¡œë”© ì‹¤íŒ¨: {e}")
        st.stop()

# LLM ì´ˆê¸°í™” (API í‚¤ ì§ì ‘ ì£¼ì… & 2.5 ëª¨ë¸ ì‚¬ìš©)
llm_text = ChatGoogleGenerativeAI(
    model=MODEL_NAME, 
    temperature=0, 
    google_api_key=GOOGLE_API_KEY
)
llm_vision = ChatGoogleGenerativeAI(
    model=MODEL_NAME, 
    temperature=0, 
    google_api_key=GOOGLE_API_KEY
)

# ==========================================================
# [2] ë¡œì§ ì²´ì¸ (RAG & Vision)
# ==========================================================
spacing_chain = ChatPromptTemplate.from_template("êµì •ëœ í•œêµ­ì–´ ë¬¸ì¥ë§Œ ì¶œë ¥(ì„¤ëª…X): {question}").pipe(llm_text).pipe(StrOutputParser())

def retrieve_and_rerank(query, top_k=5):
    # 1ì°¨ ê²€ìƒ‰
    initial_docs = hybrid_retriever.invoke(query)
    if not initial_docs: return []
    
    # 2ì°¨ ì¬ìˆœìœ„(Reranking)
    pairs = [[query, doc.page_content] for doc in initial_docs]
    scores = []
    batch_size = 16
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i : i + batch_size]
        batch_scores = reranker_model.predict(batch)
        scores.extend(batch_scores)
    
    scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in scored_docs[:top_k]]

# [í•µì‹¬] ë„ë©´ ë¶„ì„ í•¨ìˆ˜: ì‹œê³µ/í’ˆì§ˆ/ì•ˆì „ ê´€ë¦¬ ê´€ì 
def analyze_drawing_deep(image_base64, query, retrieved_docs):
    # DBì—ì„œ ì°¾ì•„ì˜¨ ì‹œë°©ì„œ/ì§€ì¹¨ ë‚´ìš© í•©ì¹˜ê¸°
    laws_text = "\n".join([f"- {d.page_content}" for d in retrieved_docs])
    
    prompt_text = f"""
    ë‹¹ì‹ ì€ ê±´ì„¤ í˜„ì¥ì˜ **ì‹œê³µ í’ˆì§ˆ ë° ì•ˆì „ ê´€ë¦¬ ì „ë¬¸ê°€(Construction CM Expert)**ì…ë‹ˆë‹¤.
    
    [ê²€í†  ìš”ì²­ ì‚¬í•­]
    {query}
    
    [ì°¸ê³  ê¸°ì¤€: ê±´ì„¤ê´€ë¦¬ ì§€ì¹¨ ë° ì‹œë°©ì„œ DB]
    {laws_text}
    
    [ì§€ì‹œì‚¬í•­]
    1. **ì‹œê³µ ê´€ì  ë¶„ì„:** ë„ë©´ì˜ ë¶€ì¬(ë²½ì²´, ìŠ¬ë˜ë¸Œ, ì°½í˜¸, ë§ˆê° ë“±)ë¥¼ ë³´ê³ , ì‹¤ì œ ì‹œê³µ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œì ì„ ì°¾ìœ¼ì„¸ìš”.
    2. **DB ê¸°ë°˜ ëŒ€ì¡°:** ìœ„ [ì°¸ê³  ê¸°ì¤€]ì— ìˆëŠ” ì‹œë°©ì„œë‚˜ ì§€ì¹¨ ë‚´ìš©ê³¼ ëŒ€ì¡°í•˜ì—¬, ì¤€ìˆ˜í•´ì•¼ í•  í’ˆì§ˆ ê¸°ì¤€ì´ë‚˜ ì•ˆì „ ìˆ˜ì¹™ì„ ì œì‹œí•˜ì„¸ìš”.
    3. **ì¤‘ì  ê²€í†  í•­ëª©:**
       - **í’ˆì§ˆ ê´€ë¦¬:** ê· ì—´(Crack), ëˆ„ìˆ˜, ë‹¨ì—´ ê²°ì†, ì¬ë£Œ ë¶„ë¦¬, ì–‘ìƒ ê´€ë¦¬ ë“±.
       - **ì•ˆì „ ê´€ë¦¬:** ì¶”ë½ ìœ„í—˜, ì¥ë¹„ ì „ë„, ê°€ì„¤ì¬ ì„¤ì¹˜ ë“± í˜„ì¥ ì•ˆì „ ìš”ì†Œ.
       - **í•˜ì ì˜ˆë°©:** ê²°ë¡œ, ì¸µê°„ ì†ŒìŒ, ë§ˆê° ë¶ˆëŸ‰ ë“± ì£¼ìš” í•˜ì í¬ì¸íŠ¸.
    4. **ê²°ê³¼ ë³´ê³  ì–‘ì‹:**
       - ğŸ› ï¸ **ì£¼ìš” ì‹œê³µ ê´€ë¦¬ í¬ì¸íŠ¸:** (ë„ë©´ì˜ íŠ¹ì • ë¶€ìœ„ì— ëŒ€í•´ ì‹œë°©ì„œ ê¸°ì¤€ ì œì‹œ)
       - âš ï¸ **ì˜ˆìƒë˜ëŠ” í’ˆì§ˆ/ì•ˆì „ ë¦¬ìŠ¤í¬:** (LH ì§€ì¹¨ ë“±ì— ê·¼ê±°í•œ ìœ„í—˜ ìš”ì†Œ ê²½ê³ )
       - ğŸ’¡ **ê°œì„  ì œì•ˆ:** (ì°¸ê³  ë¬¸í—Œì— ê¸°ë°˜í•œ ê¸°ìˆ ì  ì¡°ì–¸)
    """
    
    message = HumanMessage(content=[
        {"type": "text", "text": prompt_text},
        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}},
    ])
    
    return llm_vision.invoke([message]).content

# RAGìš© í”„ë¡¬í”„íŠ¸
answer_prompt = ChatPromptTemplate.from_messages([
    ("system", "ê±´ì„¤ ê¸°ì¤€ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. [Context]ë¥¼ ë³´ê³  ë‹µë³€í•˜ì„¸ìš”. ì¶œì²˜(Source/Article) í‘œê¸° í•„ìˆ˜.\n[Context]\n{context}"),
    ("human", "ì§ˆë¬¸: {question}")
])

def format_docs(docs):
    return "\n\n".join([f"<ì¶œì²˜: {d.metadata.get('source')} / {d.metadata.get('article')}>\n{d.page_content}" for d in docs])

rag_chain = (
    {"context": RunnableLambda(lambda x: retrieve_and_rerank(x, top_k=10)) | format_docs, "question": RunnablePassthrough()}
    | answer_prompt | llm_text | StrOutputParser()
)

# ==========================================================
# [3] ì›¹ UI êµ¬ì„±
# ==========================================================
st.title("ğŸ—ï¸ ê±´ì„¤ CM ì „ë¬¸ AI (ì‹œê³µ í’ˆì§ˆ/ì•ˆì „)")

# --- [A] ì‚¬ì´ë“œë°”: íŒŒì¼ ì—…ë¡œë“œ ---
if "last_processed_file" not in st.session_state:
    st.session_state.last_processed_file = None
if "analysis_result" not in st.session_state:
    st.session_state.analysis_result = None
if "current_image_base64" not in st.session_state: 
    st.session_state.current_image_base64 = None

with st.sidebar:
    st.header("ğŸ“‚ ë„ë©´ íˆ¬ì…êµ¬")
    st.info("ğŸ’¡ PDFë¥¼ ë„£ìœ¼ë©´ **ì „ì²´ í˜ì´ì§€**ë¥¼ ì‹œê³µ/í’ˆì§ˆ ê´€ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
    uploaded_files = st.file_uploader("ê²€í† í•  ë„ë©´ PDFë¥¼ ì„ íƒí•˜ì„¸ìš”", type=["pdf"], accept_multiple_files=True)

# --- [B] ìë™ ë¶„ì„ (ë‹¤ì¤‘ í˜ì´ì§€ ì „ì²´ ë¶„ì„) ---
if uploaded_files:
    target_file = uploaded_files[0]
    
    # ìƒˆ íŒŒì¼ì´ ë“¤ì–´ì˜¤ë©´ ë¶„ì„ ì‹œì‘
    if st.session_state.last_processed_file != target_file.name:
        st.session_state.analysis_result = "" # ì´ˆê¸°í™”
        st.session_state.last_processed_file = target_file.name
        
        # 1. PDF -> ì´ë¯¸ì§€ ë³€í™˜ (ì „ì²´ í˜ì´ì§€)
        with st.status("ğŸ“„ PDF ë³€í™˜ ë° ë¶„ì„ ì¤€ë¹„ ì¤‘...", expanded=True) as status:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(target_file.read())
                tmp_path = tmp_file.name
            try:
                # [í•µì‹¬] ì˜µì…˜ ì—†ì´ í˜¸ì¶œí•˜ë©´ ì „ì²´ í˜ì´ì§€ ë³€í™˜ë¨
                all_pages = convert_from_path(tmp_path, poppler_path=POPPLER_PATH)
                status.write(f"âœ… ì´ {len(all_pages)}ì¥ì˜ ë„ë©´ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì „ì²´ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ì´ë¯¸ì§€ ë³€í™˜ ì˜¤ë¥˜: {e}")
                st.stop()
        
        # 2. í˜ì´ì§€ë³„ ìˆœì°¨ ë°˜ë³µ ë¶„ì„ (Loop)
        full_report = f"### ğŸ—ï¸ ë„ë©´ ì‹œê³µ í’ˆì§ˆ/ì•ˆì „ ë¶„ì„ ê²°ê³¼ (ì´ {len(all_pages)}ì¥)\n**ë¶„ì„ ëŒ€ìƒ:** {target_file.name}\n\n"
        progress_bar = st.progress(0)
        
        # ê²°ê³¼ í‘œì‹œìš© ì»¨í…Œì´ë„ˆ
        result_container = st.container()

        for i, page_img in enumerate(all_pages):
            page_num = i + 1
            progress_msg = f"ğŸš€ {page_num}/{len(all_pages)} í˜ì´ì§€ ë¶„ì„ ì¤‘... (ì‹œê³µ ê¸°ì¤€ ëŒ€ì¡° ì¤‘)"
            progress_bar.progress((i + 1) / len(all_pages), text=progress_msg)
            
            # ì´ë¯¸ì§€ Base64 ë³€í™˜
            with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp_img:
                page_img.save(tmp_img.name, "JPEG")
                with open(tmp_img.name, "rb") as f:
                    img_base64 = base64.b64encode(f.read()).decode("utf-8")
            
            # ë§ˆì§€ë§‰ í˜ì´ì§€ ì´ë¯¸ì§€ëŠ” ì§ˆë¬¸ìš©ìœ¼ë¡œ ì„¸ì…˜ì— ì €ì¥
            st.session_state.current_image_base64 = img_base64
            
            # [ìˆ˜ì •] ìë™ ì§ˆë¬¸: í’ˆì§ˆ/ì•ˆì „/í•˜ì ê´€ì 
            auto_query = """
            ì´ ë„ë©´ì„ ì‹¤ì œ ì‹œê³µí•  ë•Œ ìœ ì˜í•´ì•¼ í•  'í’ˆì§ˆ ê´€ë¦¬', 'ì•ˆì „ ì‚¬ê³  ì˜ˆë°©', 'í•˜ì ë°©ì§€' ì‚¬í•­ì„ ê²€í† í•´ì¤˜.
            íŠ¹íˆ ì½˜í¬ë¦¬íŠ¸ íƒ€ì„¤, ë§ˆê° ê³µì‚¬, ë‹¨ì—´/ë°©ìˆ˜ ì‹œê³µ ì‹œ LH ì „ë¬¸ ì‹œë°©ì„œ ë° ê±´ì„¤ê´€ë¦¬ ì§€ì¹¨ì— ë”°ë¼ ì£¼ì˜í•  ì ì„ ì•Œë ¤ì¤˜.
            """
            
            # ê²€ìƒ‰ ë° ë¶„ì„ ìˆ˜í–‰
            retrieved_docs = retrieve_and_rerank(auto_query, top_k=5)
            vision_result = analyze_drawing_deep(img_base64, auto_query, retrieved_docs)
            
            # ê²°ê³¼ ëˆ„ì 
            page_content = f"""
#### ğŸ“„ Page {page_num} ë¶„ì„
{vision_result}
"""
            full_report += page_content
            
            # ì¤‘ê°„ ê²°ê³¼ í™”ë©´ í‘œì‹œ (Expanderë¡œ ì´ë¯¸ì§€ ë³´ì—¬ì£¼ê¸°)
            with result_container:
                with st.expander(f"ğŸ” ì œ {page_num}í˜ì´ì§€ ë„ë©´ & ìš”ì•½ ë³´ê¸°", expanded=False):
                    st.image(page_img, caption=f"Page {page_num}", width="stretch")
                    st.markdown(vision_result)

        progress_bar.empty()
        st.session_state.analysis_result = full_report
        st.success("âœ… ëª¨ë“  í˜ì´ì§€ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

# --- [C] ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì±„íŒ…ì°½ ë§¨ ìœ„ì— í‘œì‹œ
if st.session_state.analysis_result:
    if not st.session_state.messages or st.session_state.messages[0]["content"] != st.session_state.analysis_result:
        st.session_state.messages = [{"role": "assistant", "content": st.session_state.analysis_result}]

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- [D] ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ---
if prompt := st.chat_input("ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹ ê°€ìš”? (ì˜ˆ: ì°½í˜¸ ì£¼ìœ„ ê²°ë¡œ ë°©ì§€ ëŒ€ì±…ì€?)"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
        if st.session_state.current_image_base64:
            with st.status("ğŸ” ë„ë©´ê³¼ ì‹œë°©ì„œë¥¼ ë³µí•© ë¶„ì„ ì¤‘...", expanded=True) as status:
                st.write("ğŸ“š ê´€ë ¨ ì‹œë°©ì„œ/ì§€ì¹¨ ê²€ìƒ‰ ì¤‘...")
                retrieved_docs = retrieve_and_rerank(prompt, top_k=5)
                
                st.write("ğŸ‘€ ë§ˆì§€ë§‰ ë„ë©´ í˜ì´ì§€ ì¬í™•ì¸ ì¤‘...")
                # ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ì‹œ Vision AI í˜¸ì¶œ
                vision_res = analyze_drawing_deep(st.session_state.current_image_base64, prompt, retrieved_docs)
                status.update(label="âœ… ë‹µë³€ ì™„ë£Œ", state="complete")
            
            final_res = f"{vision_res}\n\n[ì°¸ê³  ìë£Œ]: " + ", ".join([d.metadata.get('article', 'ì¶œì²˜ë¯¸ìƒ') for d in retrieved_docs])
            st.markdown(final_res)
            st.session_state.messages.append({"role": "assistant", "content": final_res})
        
        # ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°
        else:
            corrected = spacing_chain.invoke({"question": prompt})
            response = rag_chain.invoke(corrected)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})