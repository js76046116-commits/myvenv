import streamlit as st
import os
import json
import itertools
import base64
import tempfile
import platform 
import time

# [í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬]
from pdf2image import convert_from_path
from sentence_transformers import CrossEncoder 
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory
from langchain_community.retrievers import BM25Retriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage

# ==========================================================
# [0] ê¸°ë³¸ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ==========================================================
st.set_page_config(page_title="ê±´ì„¤ CM AI í†µí•© ì†”ë£¨ì…˜ (Deep RAG)", page_icon="ğŸ—ï¸", layout="wide")

# 1. API í‚¤ ê°€ì ¸ì˜¤ê¸°
if "GOOGLE_API_KEY" in st.secrets:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
elif "GOOGLE_API_KEY" in os.environ:
    GOOGLE_API_KEY = os.environ["GOOGLE_API_KEY"]
else:
    st.error("ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: Google API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

# 2. Poppler ê²½ë¡œ (Windows í™˜ê²½ ëŒ€ì‘)
system_name = platform.system()
if system_name == "Windows":
    # ì‚¬ìš©ìì˜ ë¡œì»¬ í™˜ê²½ ê²½ë¡œ (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
    POPPLER_PATH = r"C:\Users\owner\myvenv\Release-25.12.0-0\poppler-25.12.0\Library\bin"
else:
    POPPLER_PATH = None 

# 3. ë°ì´í„° ê²½ë¡œ
DB_PATH_1 = "./chroma_db_part1"
DB_PATH_2 = "./chroma_db_part2"
JSON_DATA_PATH = "./legal_data_total_vlm.json"
RAW_DATA = []

# 4. ëª¨ë¸ ì„¤ì •
MODEL_NAME = "models/gemini-2.5-flash" 

# ==========================================================
# [1] ì‹œìŠ¤í…œ ë¡œë”© (ê²€ìƒ‰ ì—”ì§„ & ëª¨ë¸)
# ==========================================================
class SimpleHybridRetriever:
    """BM25(í‚¤ì›Œë“œ) + Chroma(ë²¡í„°) ê²°í•© ê²€ìƒ‰ê¸°"""
    def __init__(self, bm25, chroma1, chroma2, raw_data):
        self.bm25 = bm25
        self.chroma1 = chroma1
        self.chroma2 = chroma2
        self.raw_data = raw_data
        
    def invoke(self, query):
        # 1. ë³‘ë ¬ ê²€ìƒ‰ ìˆ˜í–‰
        docs_bm25 = self.bm25.invoke(query)
        docs_c1 = self.chroma1.invoke(query)
        docs_c2 = self.chroma2.invoke(query)
        
        # 2. Chroma ê²°ê³¼ ë³µì› (ì¸ë±ìŠ¤ -> ì›ë³¸ í…ìŠ¤íŠ¸)
        real_docs_chroma = []
        for doc in (docs_c1 + docs_c2):
            try:
                idx = int(doc.page_content) 
                original_item = self.raw_data[idx] 
                content = original_item.get('content', '').strip()
                source = original_item.get('source', '').strip()
                article = original_item.get('article', '').strip()
                full_text = f"[{source}] {content}"
                new_doc = Document(page_content=full_text, metadata={"source": source, "article": article})
                real_docs_chroma.append(new_doc)
            except:
                continue

        # 3. ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        combined = []
        seen_ids = set()
        for d in itertools.chain(docs_bm25, real_docs_chroma):
            key = d.page_content[:30] # ë‚´ìš© ì•ë¶€ë¶„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            if key not in seen_ids:
                combined.append(d)
                seen_ids.add(key)
        return combined[:200] # 1ì°¨ì ìœ¼ë¡œ ë„‰ë„‰í•˜ê²Œ ë°˜í™˜

@st.cache_resource
def load_search_system():
    global RAW_DATA
    if not os.path.exists(JSON_DATA_PATH):
        st.error("âŒ JSON ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
        
    with open(JSON_DATA_PATH, 'r', encoding='utf-8') as f:
        RAW_DATA = json.load(f)

    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=GOOGLE_API_KEY)
    
    if not os.path.exists(DB_PATH_1) or not os.path.exists(DB_PATH_2):
        st.error("âŒ DB í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    store1 = Chroma(persist_directory=DB_PATH_1, embedding_function=embeddings, collection_name="construction_laws")
    retriever1 = store1.as_retriever(search_kwargs={"k": 100})
    store2 = Chroma(persist_directory=DB_PATH_2, embedding_function=embeddings, collection_name="construction_laws")
    retriever2 = store2.as_retriever(search_kwargs={"k": 100})

    docs = []
    for item in RAW_DATA:
        content = item.get('content', '').strip()
        source = item.get('source', '').strip()
        if not content: continue
        doc = Document(page_content=f"[{source}] {content}", metadata={"source": source, "article": item.get('article', '')})
        docs.append(doc)
    
    bm25_retriever = BM25Retriever.from_documents(docs)
    bm25_retriever.k = 150
    hybrid_retriever = SimpleHybridRetriever(bm25_retriever, retriever1, retriever2, RAW_DATA)
    
    # Cross-Encoder (Reranker) ë¡œë“œ
    reranker = CrossEncoder("cross-encoder/ms-marco-TinyBERT-L-2-v2", model_kwargs={"dtype": "auto"})

    return hybrid_retriever, reranker

with st.spinner("ğŸš€ AI 5ë‹¨ê³„ ì‹¬ì¸µ ê²€ìƒ‰ ì—”ì§„ ì‹œë™ ì¤‘..."):
    try:
        hybrid_retriever, reranker_model = load_search_system()
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ë¡œë”© ì‹¤íŒ¨: {e}")
        st.stop()

# LLM ì´ˆê¸°í™”
safety_settings = {HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE}
llm_text = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.1, google_api_key=GOOGLE_API_KEY, safety_settings=safety_settings)
llm_vision = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0, google_api_key=GOOGLE_API_KEY, safety_settings=safety_settings)

# ==========================================================
# [2] Deep RAG íŒŒì´í”„ë¼ì¸ (5ë‹¨ê³„ ë¡œì§ êµ¬í˜„)
# ==========================================================

# (1) ì¿¼ë¦¬ í™•ì¥ (Query Expansion)
expansion_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ê±´ì„¤/ê±´ì¶• ê²€ìƒ‰ ìµœì í™” AIì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆëŠ” **'í™•ì¥ ê²€ìƒ‰ì–´'** 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.
ê±´ì„¤ í‘œì¤€ ì‹œë°©ì„œ, ë²•ê·œ ìš©ì–´, ë™ì˜ì–´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

[ì‚¬ìš©ì ì§ˆë¬¸]: {question}

[ì¶œë ¥ í˜•ì‹]: ì§ˆë¬¸ | í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3
(ì„¤ëª… ì—†ì´ ìœ„ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”)
""")
expansion_chain = expansion_prompt | llm_text | StrOutputParser()

def get_expanded_queries(original_query):
    """(1ë‹¨ê³„) ì‚¬ìš©ì ì§ˆë¬¸ì„ í™•ì¥í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    try:
        expanded_str = expansion_chain.invoke({"question": original_query})
        if "|" in expanded_str:
            base, keywords = expanded_str.split("|", 1)
            queries = [base.strip()] + [k.strip() for k in keywords.split(",")]
        else:
            queries = [original_query]
        return queries[:4] # ìµœëŒ€ 4ê°œê¹Œì§€ë§Œ ì‚¬ìš© (ì†ë„ ì¡°ì ˆ)
    except:
        return [original_query]

# (2)~(4) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ì¬ìˆœìœ„í™” + Top-K í•„í„°ë§
def retrieve_and_rerank(query, top_k=50):
    # Step 1: ì¿¼ë¦¬ í™•ì¥
    expanded_queries = get_expanded_queries(query)
    
    # Step 2: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í™•ì¥ëœ ì¿¼ë¦¬ ê°ê° ìˆ˜í–‰)
    all_docs = []
    seen_contents = set()
    
    for q in expanded_queries:
        docs = hybrid_retriever.invoke(q)
        for doc in docs:
            if doc.page_content not in seen_contents:
                all_docs.append(doc)
                seen_contents.add(doc.page_content)
    
    if not all_docs: return []

    # Step 3: ì •ë°€ ì¬ìˆœìœ„í™” (Cross-Encoder)
    pairs = [[query, doc.page_content] for doc in all_docs]
    scores = []
    batch_size = 32
    
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i : i + batch_size]
        batch_scores = reranker_model.predict(batch)
        scores.extend(batch_scores)
    
    scored_docs = sorted(zip(all_docs, scores), key=lambda x: x[1], reverse=True)
    
    # Step 4: Top-K í•„í„°ë§ (Top-50)
    final_top_k = [doc for doc, score in scored_docs[:top_k]]
    return final_top_k

# (5) ë‹µë³€ ìƒì„± (ìœ ì—°í•œ í”„ë¡¬í”„íŠ¸)
spacing_chain = ChatPromptTemplate.from_template("êµì •ëœ í•œêµ­ì–´ ë¬¸ì¥ë§Œ ì¶œë ¥(ì„¤ëª…X): {question}").pipe(llm_text).pipe(StrOutputParser())

answer_prompt = ChatPromptTemplate.from_messages([
    ("system", """
    ë‹¹ì‹ ì€ ë² í…Œë‘ ê±´ì„¤ ì‚¬ì—… ê´€ë¦¬ì(CM)ì´ì ì‹œê³µ ê¸°ìˆ ì‚¬ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ [Context](ê²€ìƒ‰ëœ ë²•ê·œ/ì‹œë°©ì„œ)ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

    [ë‹µë³€ ê·œì¹™]
    1. **ìš°ì„  ìˆœìœ„:** [Context]ì— êµ¬ì²´ì ì¸ ì ˆì°¨ë‚˜ ê¸°ì¤€ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ê²ƒì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    2. **ì¼ë°˜ ì§€ì‹ í™œìš©:** ë§Œì•½ [Context]ì— 'í•´ê²° ë°©ì•ˆ'ì´ë‚˜ 'êµ¬ì²´ì  ê³µë²•'ì´ ë¶€ì¡±í•˜ë‹¤ë©´, 
       **"ì œê³µëœ ë²•ê·œ ë°ì´í„°ì—ëŠ” êµ¬ì²´ì  ë°©ë²•ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë‚˜, ì¼ë°˜ì ì¸ ì‹œê³µ ê¸°ì¤€ì— ë”°ë¥´ë©´..."** ì´ë¼ê³  ì–¸ê¸‰í•œ ë’¤, ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” **í‘œì¤€ ì‹œë°©ì„œ ë° ê³µí•™ì  ì§€ì‹**ì„ ë™ì›í•´ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”.
    3. ì ˆëŒ€ "ëª¨ë¥¸ë‹¤"ê³  ëë‚´ì§€ ë§ê³ , ì‹¤ë¬´ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
    4. ì¶œì²˜ê°€ ìˆë‹¤ë©´ [ì¶œì²˜: ...] í˜•íƒœë¡œ ëª…ì‹œí•˜ì„¸ìš”.

    [Context]
    {context}
    """),
    ("human", "ì§ˆë¬¸: {question}")
])

def format_docs(docs):
    return "\n\n".join([f"<ì¶œì²˜: {d.metadata.get('source')} / {d.metadata.get('article')}>\n{d.page_content}" for d in docs])

# ìµœì¢… RAG ì²´ì¸ (Top-50 ì ìš©)
rag_chain = (
    {"context": RunnableLambda(lambda x: retrieve_and_rerank(x, top_k=50)) | format_docs, "question": RunnablePassthrough()}
    | answer_prompt | llm_text | StrOutputParser()
)

# ==========================================================
# [3] Vision AI (ë„ë©´ ë¶„ì„ìš©)
# ==========================================================
def analyze_page_detail(image_base64, query, retrieved_docs):
    laws_text = "\n".join([f"- {d.page_content[:200]}..." for d in retrieved_docs])
    if not laws_text.strip():
        laws_text = "(ì¼ë°˜ ì‹œê³µ ì§€ì‹ ê¸°ë°˜)"

    prompt_text = f"""
    ë‹¹ì‹ ì€ ê±´ì„¤ ì‹œê³µ í’ˆì§ˆ/ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    [ê²€í†  ìš”ì²­] {query}
    [ì°¸ê³  ê¸°ì¤€] {laws_text}
    
    ë„ë©´ ì´ë¯¸ì§€ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬ í’ˆì§ˆ ë¬¸ì œ(ê· ì—´, ëˆ„ìˆ˜ ë“±)ì™€ ì•ˆì „ ìœ„í—˜ì„ ì°¾ì•„ë‚´ì„¸ìš”.
    """
    
    message = HumanMessage(content=[
        {"type": "text", "text": prompt_text},
        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}},
    ])
    try:
        response = llm_vision.invoke([message])
        return response.content
    except:
        return "ë¶„ì„ ë¶ˆê°€"

def generate_final_report(file_name, page_results):
    raw_data = ""
    for item in page_results:
        raw_data += f"\n[Page {item['page']}]: {item['content']}\n"
    
    prompt = f"""
    ë‹¹ì‹ ì€ ê±´ì„¤ì‚¬ì—…ê´€ë¦¬ë‹¨ì¥(CMë‹¨ì¥)ì…ë‹ˆë‹¤.
    '{file_name}' ë„ë©´ì˜ í˜ì´ì§€ë³„ ë¶„ì„ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ **ìµœì¢… ì‹œê³µ í’ˆì§ˆ/ì•ˆì „ ê²€í†  ë³´ê³ ì„œ**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    ì¤‘ë³µëœ ë‚´ìš©ì€ í†µí•©í•˜ê³ , í•µì‹¬ ì´ìŠˆ ìœ„ì£¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
    
    [ë¶„ì„ ë°ì´í„°]
    {raw_data}
    """
    return llm_text.invoke(prompt).content

# ==========================================================
# [4] ì›¹ UI (Streamlit)
# ==========================================================
st.title("ğŸ—ï¸ ê±´ì„¤ CM ì „ë¬¸ AI (Deep RAG + Vision)")

# ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processed_files" not in st.session_state:
    st.session_state.processed_files = set()
if "current_image_base64" not in st.session_state:
    st.session_state.current_image_base64 = None

# --- [ì‚¬ì´ë“œë°”] íŒŒì¼ ì—…ë¡œë“œ ë° ëª¨ë“œ ì„¤ì • ---
with st.sidebar:
    st.header("ğŸ“‚ ë„ë©´ íˆ¬ì…êµ¬")
    uploaded_files = st.file_uploader("PDF ë„ë©´ ì—…ë¡œë“œ", type=["pdf"], accept_multiple_files=True)
    
    st.markdown("---")
    
    # [í•µì‹¬] íŒŒì¼ ì—…ë¡œë“œ ì—¬ë¶€ì— ë”°ë¼ UI ì¦‰ì‹œ ë³€ê²½
    if uploaded_files:
        st.subheader("ğŸ¤– ì§ˆë¬¸ ëª¨ë“œ")
        search_mode = st.radio(
            "ëª¨ë“œ ì„ íƒ",
            ["ğŸ“‚ ë„ë©´ ê¸°ë°˜ ì§ˆë¬¸", "âš–ï¸ ì¼ë°˜ ë²•ê·œ ê²€ìƒ‰"],
            index=0,
            help="ğŸ“‚ ë„ë©´ ê¸°ë°˜: í˜„ì¬ ë³´ëŠ” ë„ë©´ ë‚´ìš© ì°¸ê³ \nâš–ï¸ ì¼ë°˜ ë²•ê·œ: ë„ë©´ ë¬´ì‹œ, ë²•ê·œ DB ì‹¬ì¸µ ê²€ìƒ‰"
        )
    else:
        # íŒŒì¼ì´ ì—†ì„ ë•Œ ê¸°ë³¸ê°’
        search_mode = "âš–ï¸ ì¼ë°˜ ë²•ê·œ ê²€ìƒ‰"
        st.info("ğŸ’¡ ë„ë©´ì´ ì—†ìŠµë‹ˆë‹¤.\n**'ì‹¬ì¸µ ë²•ê·œ ê²€ìƒ‰ ëª¨ë“œ'**ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n(Query Expansion + Rerank)")

# --- [ë©”ì¸] ë„ë©´ ì²˜ë¦¬ ë¡œì§ ---
if uploaded_files:
    for target_file in uploaded_files:
        if target_file.name not in st.session_state.processed_files:
            with st.status(f"ğŸ“„ '{target_file.name}' ë¶„ì„ ì¤‘...", expanded=True) as status:
                # 1. PDF ë³€í™˜
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(target_file.read())
                    tmp_path = tmp_file.name
                
                try:
                    all_pages = convert_from_path(tmp_path, poppler_path=POPPLER_PATH)
                except Exception as e:
                    st.error(f"ë³€í™˜ ì˜¤ë¥˜: {e}")
                    continue

                # 2. Vision ë¶„ì„ ë£¨í”„
                page_results = []
                progress = st.progress(0)
                for i, page_img in enumerate(all_pages):
                    progress.progress((i+1)/len(all_pages), text=f"ğŸ” Page {i+1} ì •ë°€ ì§„ë‹¨ ì¤‘...")
                    
                    # ì´ë¯¸ì§€ base64 ë³€í™˜
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp_img:
                        page_img.save(tmp_img.name, "JPEG")
                        with open(tmp_img.name, "rb") as f:
                            img_base64 = base64.b64encode(f.read()).decode("utf-8")
                    
                    st.session_state.current_image_base64 = img_base64 # ìµœì‹  ì´ë¯¸ì§€ ì €ì¥
                    
                    # ë¶„ì„ ì‹¤í–‰
                    res = analyze_page_detail(img_base64, "ìœ„í—˜ ìš”ì†Œ ì‹ë³„", [])
                    page_results.append({"page": i+1, "content": res})
                
                # 3. ì¢…í•© ë³´ê³ ì„œ
                status.write("ğŸ“ ì¢…í•© ë³´ê³ ì„œ ì‘ì„± ì¤‘...")
                report = generate_final_report(target_file.name, page_results)
                
                st.session_state.processed_files.add(target_file.name)
                st.session_state.messages.append({"role": "assistant", "content": report})
                progress.empty()
                status.update(label="ë¶„ì„ ì™„ë£Œ", state="complete")

# --- [ì±„íŒ…ì°½] ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # [ë¶„ê¸° ë¡œì§] ë„ë©´ ëª¨ë“œ vs ë²•ê·œ ëª¨ë“œ
        
        # Case 1: ë„ë©´ ëª¨ë“œì´ê³  + ì´ë¯¸ì§€ê°€ ìˆì„ ë•Œ -> Vision AI
        if search_mode == "ğŸ“‚ ë„ë©´ ê¸°ë°˜ ì§ˆë¬¸" and st.session_state.current_image_base64:
            with st.status("ğŸ” ë„ë©´ ì¬ê²€í†  ë° Vision ë¶„ì„ ì¤‘...", expanded=True):
                msg = HumanMessage(content=[
                    {"type": "text", "text": f"ì§ˆë¬¸: {prompt}\n(ì´ì „ ë¶„ì„ ë§¥ë½ ì°¸ê³ í•˜ì—¬ ë‹µë³€)"},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{st.session_state.current_image_base64}"}}
                ])
                response = llm_vision.invoke([msg]).content
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
        
        # Case 2: ì¼ë°˜ ë²•ê·œ ëª¨ë“œì´ê±°ë‚˜ OR ë„ë©´ì´ ì—†ì„ ë•Œ -> Deep RAG
        else:
            with st.status("ğŸ§  5ë‹¨ê³„ ì‹¬ì¸µ ê²€ìƒ‰ ì¤‘ (í™•ì¥-ê²€ìƒ‰-ì¬ìˆœìœ„í™”)...", expanded=True):
                # ì¿¼ë¦¬ êµì • ë° í™•ì¥ -> ê²€ìƒ‰ -> ë‹µë³€
                corrected_query = spacing_chain.invoke({"question": prompt})
                response = rag_chain.invoke(corrected_query)
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})